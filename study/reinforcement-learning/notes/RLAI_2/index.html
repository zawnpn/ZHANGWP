



<!DOCTYPE html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Welcome to zawnpn's blog.">
      
      
        <link rel="canonical" href="https://oncemath.com/study/reinforcement-learning/notes/RLAI_2/">
      
      
        <meta name="author" content="zawnpn">
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="en, jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="/assets/images/all_inclusive.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Chapter 2 - ONCE.MATH</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../../../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
    
    <link rel="stylesheet" href="../../../../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="light-blue">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../../../../#reinforcement-learning-an-introduction-chapter-2" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://oncemath.com" title="ONCE.MATH" class="md-header-nav__button md-logo">
          
            <i class="md-icon">all_inclusive</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                ONCE.MATH
              </span>
              <span class="md-header-nav__topic">
                Chapter 2
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/zawnpn/ONCEMATH/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../.." title="Home" class="md-tabs__link">
          Home
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../" title="Study" class="md-tabs__link md-tabs__link--active">
          Study
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../projects/" title="Projects" class="md-tabs__link">
          Projects
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../math/" title="Math" class="md-tabs__link">
          Math
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../diary/" title="Diary" class="md-tabs__link">
          Diary
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../share/" title="Share" class="md-tabs__link">
          Share
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../about/" title="About" class="md-tabs__link">
          About
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://oncemath.com" title="ONCE.MATH" class="md-nav__button md-logo">
      
        <i class="md-icon">all_inclusive</i>
      
    </a>
    ONCE.MATH
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/zawnpn/ONCEMATH/" title="前往 Github 仓库" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      Home
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        Home
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../.." title="主页" class="md-nav__link">
      主页
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../links/" title="友情链接" class="md-nav__link">
      友情链接
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../donates/" title="打赏" class="md-nav__link">
      打赏
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Study
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Study
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../" title="README" class="md-nav__link">
      README
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2" checked>
    
    <label class="md-nav__link" for="nav-2-2">
      Reinforcement
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        Reinforcement
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2-1" type="checkbox" id="nav-2-2-1" checked>
    
    <label class="md-nav__link" for="nav-2-2-1">
      Reinforcement Learning An Introduction
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-2-1">
        Reinforcement Learning An Introduction
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Chapter 2
      </label>
    
    <a href="./" title="Chapter 2" class="md-nav__link md-nav__link--active">
      Chapter 2
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#21-a-k-armed-bandit-problem" title="2.1 A k-armed Bandit Problem" class="md-nav__link">
    2.1 A k-armed Bandit Problem
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#background" title="Background" class="md-nav__link">
    Background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#representation-of-the-problem" title="Representation of the problem" class="md-nav__link">
    Representation of the problem
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-action-value-methods" title="2.2 Action-value Methods" class="md-nav__link">
    2.2 Action-value Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sample-average" title="Sample-Average" class="md-nav__link">
    Sample-Average
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#greedy-action" title="greedy action" class="md-nav__link">
    greedy action
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-greedy-action" title="ε-greedy action" class="md-nav__link">
    ε-greedy action
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23-the-10-armed-testbed" title="2.3 The 10-armed Testbed" class="md-nav__link">
    2.3 The 10-armed Testbed
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#background_1" title="Background" class="md-nav__link">
    Background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conclusion" title="Conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#24-incremental-implementation" title="2.4 Incremental Implementation" class="md-nav__link">
    2.4 Incremental Implementation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimization" title="Optimization" class="md-nav__link">
    Optimization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pseuducode" title="Pseuducode" class="md-nav__link">
    Pseuducode
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25-tracking-a-nonstationary-problem" title="2.5 Tracking a Nonstationary Problem" class="md-nav__link">
    2.5 Tracking a Nonstationary Problem
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26-optimistic-initial-values" title="2.6 Optimistic Initial Values" class="md-nav__link">
    2.6 Optimistic Initial Values
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#27-upper-confidence-bound-action-selection" title="2.7 Upper-Confidence-Bound Action Selection" class="md-nav__link">
    2.7 Upper-Confidence-Bound Action Selection
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#28-gradient-bandit-algorithms" title="2.8 Gradient Bandit Algorithms" class="md-nav__link">
    2.8 Gradient Bandit Algorithms
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction" title="Introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#proof" title="Proof" class="md-nav__link">
    Proof
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#29-associative-search-contextual-bandit" title="2.9 Associative Search (Contextual Bandit)" class="md-nav__link">
    2.9 Associative Search (Contextual Bandit)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#background_2" title="Background" class="md-nav__link">
    Background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-reinforcement-learning-problem" title="Full Reinforcement Learning Problem" class="md-nav__link">
    Full Reinforcement Learning Problem
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Projects
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Projects
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../projects/" title="README" class="md-nav__link">
      README
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../projects/FFT-GPU-Accel/" title="基于GPU的快速傅里叶变换并行化加速" class="md-nav__link">
      基于GPU的快速傅里叶变换并行化加速
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3-3" type="checkbox" id="nav-3-3">
    
    <label class="md-nav__link" for="nav-3-3">
      Steam-Toolkit
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-3-3">
        Steam-Toolkit
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../projects/steam-market-price-bot/" title="Steam市场比价爬虫" class="md-nav__link">
      Steam市场比价爬虫
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3-4" type="checkbox" id="nav-3-4">
    
    <label class="md-nav__link" for="nav-3-4">
      NKU-Toolkit
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-3-4">
        NKU-Toolkit
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../projects/nku-eamis/" title="NKU-EAMIS工具" class="md-nav__link">
      NKU-EAMIS工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../projects/nku-sms-rss/" title="NKU-SMS-RSS" class="md-nav__link">
      NKU-SMS-RSS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../projects/eamis-miniapp/" title="NKU-EAMIS_MiniApp(南开大学教务助手小程序)" class="md-nav__link">
      NKU-EAMIS_MiniApp(南开大学教务助手小程序)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../projects/eamis-workflow/" title="NKU-EAMIS for iOS(Workflow)" class="md-nav__link">
      NKU-EAMIS for iOS(Workflow)
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Math
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Math
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/" title="README" class="md-nav__link">
      README
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2">
    
    <label class="md-nav__link" for="nav-4-2">
      数学建模
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-2">
        数学建模
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/2017-mcm-icm/" title="2017美赛参赛整理(Problem D)" class="md-nav__link">
      2017美赛参赛整理(Problem D)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/2016-guosai/" title="2016数学建模国赛" class="md-nav__link">
      2016数学建模国赛
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/math-model-szb/" title="数学建模之2016深圳杯——初次尝试" class="md-nav__link">
      数学建模之2016深圳杯——初次尝试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/polygon-to-ellipse/" title="随机多边形转化为椭圆的过程研究" class="md-nav__link">
      随机多边形转化为椭圆的过程研究
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3" type="checkbox" id="nav-4-3">
    
    <label class="md-nav__link" for="nav-4-3">
      NKU数院试题整理
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-3">
        NKU数院试题整理
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-1" type="checkbox" id="nav-4-3-1">
    
    <label class="md-nav__link" for="nav-4-3-1">
      分析
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-3-1">
        分析
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/functional-analysis-final/" title="2017-2018第一学期泛函分析期末考试" class="md-nav__link">
      2017-2018第一学期泛函分析期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/real-variable-function/" title="2016-2017第二学期实变函数期末考试" class="md-nav__link">
      2016-2017第二学期实变函数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/mathematical-analysis-3-3-final/" title="2016-2017第一学期数学分析3-3期末考试" class="md-nav__link">
      2016-2017第一学期数学分析3-3期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/complex-analysis-final/" title="2016-2017第一学期复变函数期末考试" class="md-nav__link">
      2016-2017第一学期复变函数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/mathematical-analysis-3-3-middle/" title="2016-2017第一学期数学分析3-3期中考试" class="md-nav__link">
      2016-2017第一学期数学分析3-3期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/mathematical-analysis-3-2-final/" title="2015-2016第二学期数学分析3-2期末考试（含解答）" class="md-nav__link">
      2015-2016第二学期数学分析3-2期末考试（含解答）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/mathematical-analysis-3-2-middle/" title="2015-2016第二学期数学分析3-2期中考试" class="md-nav__link">
      2015-2016第二学期数学分析3-2期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/mathematical-analysis-3-1-final/" title="2015-2016第一学期数学分析3-1期末考试" class="md-nav__link">
      2015-2016第一学期数学分析3-1期末考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-2" type="checkbox" id="nav-4-3-2">
    
    <label class="md-nav__link" for="nav-4-3-2">
      代数
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-3-2">
        代数
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/abstract-algebra-final/" title="2016-2017第一学期抽象代数期末考试" class="md-nav__link">
      2016-2017第一学期抽象代数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/abstract-algebra-middle/" title="2016-2017第一学期抽象代数期中考试" class="md-nav__link">
      2016-2017第一学期抽象代数期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/advanced-algebra-2-2-final/" title="2015-2016第二学期高等代数2-2期末考试" class="md-nav__link">
      2015-2016第二学期高等代数2-2期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/advanced-algebra-2-2-middle/" title="2015-2016第二学期高等代数2-2期中考试" class="md-nav__link">
      2015-2016第二学期高等代数2-2期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/advanced-algebra-2-1-final/" title="2015-2016第一学期高等代数2-1期末考试" class="md-nav__link">
      2015-2016第一学期高等代数2-1期末考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-3" type="checkbox" id="nav-4-3-3">
    
    <label class="md-nav__link" for="nav-4-3-3">
      概率统计
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-3-3">
        概率统计
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/probability-final/" title="2016-2017第二学期概率论期末考试" class="md-nav__link">
      2016-2017第二学期概率论期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/probability-middle/" title="2016-2017第二学期概率论期中考试" class="md-nav__link">
      2016-2017第二学期概率论期中考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-4" type="checkbox" id="nav-4-3-4">
    
    <label class="md-nav__link" for="nav-4-3-4">
      微分方程
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-3-4">
        微分方程
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/PDE-final/" title="2017-2018第一学期数理方程期末考试" class="md-nav__link">
      2017-2018第一学期数理方程期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/ODE-final/" title="2016-2017第一学期常微分方程期末考试" class="md-nav__link">
      2016-2017第一学期常微分方程期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../math/exam/ODE-middle/" title="2016-2017第一学期常微分方程期中考试" class="md-nav__link">
      2016-2017第一学期常微分方程期中考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Diary
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Diary
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../diary/" title="README" class="md-nav__link">
      README
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../diary/roc-fly/" title="鹏程万里" class="md-nav__link">
      鹏程万里
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../diary/blog-history/" title="博客历史" class="md-nav__link">
      博客历史
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      Share
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        Share
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/" title="README" class="md-nav__link">
      README
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/nku-sms-exams/" title="NKU数院试题整理" class="md-nav__link">
      NKU数院试题整理
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/my-postgraduate-share/" title="保研推免经验分享-计算机方向（跨专业）" class="md-nav__link">
      保研推免经验分享-计算机方向（跨专业）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/github-student-pack/" title="Student Developer Pack - GitHub Education" class="md-nav__link">
      Student Developer Pack - GitHub Education
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-7" type="checkbox" id="nav-7">
    
    <label class="md-nav__link" for="nav-7">
      About
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-7">
        About
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../about/" title="关于我" class="md-nav__link">
      关于我
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#21-a-k-armed-bandit-problem" title="2.1 A k-armed Bandit Problem" class="md-nav__link">
    2.1 A k-armed Bandit Problem
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#background" title="Background" class="md-nav__link">
    Background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#representation-of-the-problem" title="Representation of the problem" class="md-nav__link">
    Representation of the problem
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-action-value-methods" title="2.2 Action-value Methods" class="md-nav__link">
    2.2 Action-value Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sample-average" title="Sample-Average" class="md-nav__link">
    Sample-Average
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#greedy-action" title="greedy action" class="md-nav__link">
    greedy action
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-greedy-action" title="ε-greedy action" class="md-nav__link">
    ε-greedy action
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23-the-10-armed-testbed" title="2.3 The 10-armed Testbed" class="md-nav__link">
    2.3 The 10-armed Testbed
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#background_1" title="Background" class="md-nav__link">
    Background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conclusion" title="Conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#24-incremental-implementation" title="2.4 Incremental Implementation" class="md-nav__link">
    2.4 Incremental Implementation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimization" title="Optimization" class="md-nav__link">
    Optimization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pseuducode" title="Pseuducode" class="md-nav__link">
    Pseuducode
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25-tracking-a-nonstationary-problem" title="2.5 Tracking a Nonstationary Problem" class="md-nav__link">
    2.5 Tracking a Nonstationary Problem
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26-optimistic-initial-values" title="2.6 Optimistic Initial Values" class="md-nav__link">
    2.6 Optimistic Initial Values
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#27-upper-confidence-bound-action-selection" title="2.7 Upper-Confidence-Bound Action Selection" class="md-nav__link">
    2.7 Upper-Confidence-Bound Action Selection
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#28-gradient-bandit-algorithms" title="2.8 Gradient Bandit Algorithms" class="md-nav__link">
    2.8 Gradient Bandit Algorithms
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction" title="Introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#proof" title="Proof" class="md-nav__link">
    Proof
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#29-associative-search-contextual-bandit" title="2.9 Associative Search (Contextual Bandit)" class="md-nav__link">
    2.9 Associative Search (Contextual Bandit)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#background_2" title="Background" class="md-nav__link">
    Background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-reinforcement-learning-problem" title="Full Reinforcement Learning Problem" class="md-nav__link">
    Full Reinforcement Learning Problem
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/zawnpn/ONCEMATH/edit/master/docs/study/reinforcement-learning/notes/RLAI_2.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="reinforcement-learning-an-introduction-chapter-2">[读书笔记]Reinforcement Learning: An Introduction - Chapter 2<a class="headerlink" href="#reinforcement-learning-an-introduction-chapter-2" title="Permanent link">&para;</a></h1>
<p>本章主要针对“非关联性（nonassociative）”的简单场景来学习基础的强化学习方法。什么是“非关联性”呢？其实通过最后 2.9 节可以看出，非关联性在本章就是指<strong>无需考虑每一步行动之间的影响，以及环境对行动的影响</strong>。非关联性问题是一种很理想化的问题，研究这种问题对于现实中的实用性意义不大，但对于入门强化学习理论，是一个不错的背景载体。</p>
<p>再说到强化学习，他与其他的机器学习方法最大的区别，也就是他自身的特点，在于强化学习重点关注<strong>评价性反馈（Evaluative Feedback）</strong>，而不是<strong>指导性反馈（Instructive Feedback）</strong>。</p>
<ul>
<li>评价性反馈：知道每一步 action 的好坏程度，但不知道这个 action 是否是最好/最差</li>
<li>指导性反馈：直接得知最优 action</li>
</ul>
<p>指导性反馈多用在监督学习中，需要大量正确的先验知识/信息来给予“指导”，而在一些特殊背景下，无法得到监督性指导，但却有大量实时的评价性反馈，这时候就需要用到<strong>强化学习</strong>。</p>
<h2 id="21-a-k-armed-bandit-problem">2.1 A k-armed Bandit Problem<a class="headerlink" href="#21-a-k-armed-bandit-problem" title="Permanent link">&para;</a></h2>
<h3 id="background">Background<a class="headerlink" href="#background" title="Permanent link">&para;</a></h3>
<p>问题的背景就是简化的 k 臂老虎机：</p>
<ul>
<li>每次在 k 个选项中做出一个选择，称之为一个 action</li>
<li>每次根据玩家的 action 反馈一个“奖励值”，每种 action 对应的奖励值服从一个固定的概率分布（这个概率分布是我们从背后分析问题，也就是从上帝视角才能得知的，真正的玩家一开始根本不知道奖励值服从什么规律或者是否有规律，他需要通过“学习”来找到这一规律）</li>
<li>玩家的目标在于使收获的奖励的<strong>累积值</strong>最大化</li>
</ul>
<h3 id="representation-of-the-problem">Representation of the problem<a class="headerlink" href="#representation-of-the-problem" title="Permanent link">&para;</a></h3>
<div>
<div class="MathJax_Preview">q_*(a)\doteq\mathbb{E}[R_t|A_t=a]</div>
<script type="math/tex; mode=display">q_*(a)\doteq\mathbb{E}[R_t|A_t=a]</script>
</div>
<ul>
<li><span><span class="MathJax_Preview">A_t</span><script type="math/tex">A_t</script></span>: 第 t 步做出的 action</li>
<li><span><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span>: 第 t 步行动后得到的回报值</li>
<li><span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>: 一个任意的行动</li>
<li><span><span class="MathJax_Preview">q_*(a)</span><script type="math/tex">q_*(a)</script></span>: 行动 a 的理论期望值</li>
</ul>
<p>我们可以很自然地想到，如果玩家真的从上帝视角得知了这台老虎机的回报规律，也即是知道了每个行动 a 真正能得到理论期望回报<span><span class="MathJax_Preview">q_*(a)</span><script type="math/tex">q_*(a)</script></span>，那只需保持选择能够收获最大期望值的 action ，就能确保最大的总收益。所以，这个问题的目标，就是要去学习探索，取得关于 <span><span class="MathJax_Preview">q_*(a), \forall a</span><script type="math/tex">q_*(a), \forall a</script></span> 的信息。</p>
<p>但是玩家一开始显然是不知道 <span><span class="MathJax_Preview">q_*(a)</span><script type="math/tex">q_*(a)</script></span> 的情况，所以他要建立一套自己对所有 <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 的评估体系，即根据他目前拥有的知识，来估计/猜测当前第 t 步 <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 的回报值 <span><span class="MathJax_Preview">Q_t(a)</span><script type="math/tex">Q_t(a)</script></span>: <span><span class="MathJax_Preview">Q_t(a)\approx q_*(a)</span><script type="math/tex">Q_t(a)\approx q_*(a)</script></span>。如何去估计呢？这个先不急，这正是后面长篇大论的东西，简言之，关键在于要有这么一套合适的评估体系。</p>
<p>先假设玩家建立好了一套他认为合适的评估体系，那接下来该如何去根据<strong>评价性反馈</strong>来采取行动呢？这时候先要提到两个概念：</p>
<ul>
<li>利用（Exploiting）：采取贪心行动，也就是根据目前<strong>已掌握的信息</strong>来做当前最优选择</li>
<li>探索（Exploring）：放弃贪心行动，去探索潜在的、有长远价值的信息</li>
</ul>
<p>Exploitation 对于每一步而言，是能尽量利用上当前已掌握知识的最佳策略，能确保回报玩家认知范围内的最佳奖励值；Exploration 则会去“试错”，去尝试一些信息量少的 action ，这些 action 之所以信息量少，是因为在玩家的评估体系中被认为是低回报 action 而很少被选中，从而收获到的信息少。不过这个低回报，既有可能是真的低回报，也有可能是被低估了，如果这个 action 事实上是一个很有价值的 action ，却因过分低估而被玩家放弃，是一件非常可惜的事情。为了避免这一情况，从长远意义上真正地最大化收益，就需要玩家适当地去探索、去试错。信息越多，做出的选择也越客观。</p>
<h2 id="22-action-value-methods">2.2 Action-value Methods<a class="headerlink" href="#22-action-value-methods" title="Permanent link">&para;</a></h2>
<h3 id="sample-average">Sample-Average<a class="headerlink" href="#sample-average" title="Permanent link">&para;</a></h3>
<p>上一节提到，玩家需要建立一套合适的评估体系，这一节就会介绍一种最简单基础的方法。</p>
<p>一个很自然的想法便是将过去得到过的奖励值取均值作为这一次对该 action 的评估</p>
<div>
<div class="MathJax_Preview">Q_{t}(a) \doteq \frac{\sum_{i=1}^{t-1}R_{i} \cdot \textbf{1}_{A_{i} = a}}{\sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}}</div>
<script type="math/tex; mode=display">Q_{t}(a) \doteq \frac{\sum_{i=1}^{t-1}R_{i} \cdot \textbf{1}_{A_{i} = a}}{\sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}}</script>
</div>
<p>我们可以看出：</p>
<ul>
<li>如果 <span><span class="MathJax_Preview">\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a} = 0</span><script type="math/tex">\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a} = 0</script></span>, 分母为 0 该式无意义，这时候需要将 <span><span class="MathJax_Preview">Q_t(a)</span><script type="math/tex">Q_t(a)</script></span> 定义为一个默认值，比如 0</li>
<li>如果 <span><span class="MathJax_Preview">\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</span><script type="math/tex">\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</script></span>, 根据<strong>大数定律</strong>, <span><span class="MathJax_Preview">Q_t(a) \rightarrow q_*(a)</span><script type="math/tex">Q_t(a) \rightarrow q_*(a)</script></span>，样本统计值收敛于理论值，达到了我们前面提到的情况——只要掌握了真实的<span><span class="MathJax_Preview">q_*(a)</span><script type="math/tex">q_*(a)</script></span>，必将能取得最优解。</li>
</ul>
<h3 id="greedy-action">greedy action<a class="headerlink" href="#greedy-action" title="Permanent link">&para;</a></h3>
<p>而前面所提到的贪心行动，表述为数学语言即为</p>
<div>
<div class="MathJax_Preview">A_t \doteq \mathop{\arg\max}\limits_aQ_t(a)</div>
<script type="math/tex; mode=display">A_t \doteq \mathop{\arg\max}\limits_aQ_t(a)</script>
</div>
<p>可以想象，纯贪心行动很有可能陷入局部最优解（最坏情况下，贪心行动甚至可能导致玩家从头到尾都在选择一个固定的非最优的 action），很难实现让每个 action 都能满足 <span><span class="MathJax_Preview">\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</span><script type="math/tex">\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</script></span> 。</p>
<p>这时候，就需要去“探索（Exploring）”，牺牲一点眼前的利益，换来能带来长远价值的信息。只需对贪心策略稍作修改，我们就能做到这一点。</p>
<h3 id="-greedy-action">ε-greedy action<a class="headerlink" href="#-greedy-action" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>ε-greedy action:</strong> 以 1-ε 的概率采取贪心行动，ε 概率随机选择一个行动 <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 。</p>
</blockquote>
<p>为什么这个 ε-greedy action 就要比单纯的 greedy action 策略好呢？我们来简单分析一下：</p>
<ul>
<li>显然可知， ε-greedy action 由于有随机探索的过程，必然能保证：当 <span><span class="MathJax_Preview">t \rightarrow \infty</span><script type="math/tex">t \rightarrow \infty</script></span>，就有 <span><span class="MathJax_Preview">\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</span><script type="math/tex">\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</script></span>。这样正如前面已经分析过的，根据大数定律会有 <span><span class="MathJax_Preview">Q_t(a)\rightarrow q_*(a)</span><script type="math/tex">Q_t(a)\rightarrow q_*(a)</script></span>.</li>
<li><span><span class="MathJax_Preview">\mathrm{Pr}\{A_t=\mathop{\arg\max}\limits_{a}Q_t(a)\} = 1-\varepsilon</span><script type="math/tex">\mathrm{Pr}\{A_t=\mathop{\arg\max}\limits_{a}Q_t(a)\} = 1-\varepsilon</script></span>，如果 ε 取得太大，就会过于注重探索，而没有充分利用好这些收获到的信息来增加我们的收益，对于我们想要最大化<strong>累积收益</strong>的目标是不利的，但如果取到一个合适的 ε ，便能兼顾信息探索和信息的<strong>充分利用</strong>。</li>
</ul>
<h2 id="23-the-10-armed-testbed">2.3 The 10-armed Testbed<a class="headerlink" href="#23-the-10-armed-testbed" title="Permanent link">&para;</a></h2>
<p>这一节就是关于上面提到的方法进行 10-armed bandit 实验来测试效果。</p>
<h3 id="background_1">Background<a class="headerlink" href="#background_1" title="Permanent link">&para;</a></h3>
<p>为了确保实验结果的准确性，总共随机生成了 2000 个 k-armed bandit 问题（k=10），然后针对每个问题，在其背景下都要进行 1000 步 action 的选择，最终针对这 2000 个独立的实验的结果来逐步取均值分析。</p>
<div align="center"><img src="../imgs/RLAI_2/10-armed.png" width="450" alt="10-armed" /></div>

<p>这个图需要好好理解一下，也要根据这张图好好再理解一下问题背景。其中，</p>
<ul>
<li>首先，需要理解的是，玩家每一步得到的奖励值 <span><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> ，是一个来自于对应的正态分布的随机值。举个例子，玩家在第 t 步选择 action 3 ，那么这一步老虎机返回给玩家的奖励值 <span><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> 就是一个服从正态分布 <span><span class="MathJax_Preview">\mathrm{N}(q_*(3),1)</span><script type="math/tex">\mathrm{N}(q_*(3),1)</script></span> 的随机值，这个值或高或低，但总体的趋势还是大概率为 <span><span class="MathJax_Preview">q_*(3)</span><script type="math/tex">q_*(3)</script></span> 附近的一个值。我们从上帝视角是知道这些值的，但是玩家并不知道这些情况，只能一步一步地收集信息，以此来猜测、学习这些奖励值的规律。</li>
<li>然后，这些 <span><span class="MathJax_Preview">q_*(a)</span><script type="math/tex">q_*(a)</script></span> 是多少呢？这些 <span><span class="MathJax_Preview">q_*(a)</span><script type="math/tex">q_*(a)</script></span> 是我们在初始生成 2000 个问题时随机定下来的，我们从标准正态分布 <span><span class="MathJax_Preview">\mathrm{N}(0,1)</span><script type="math/tex">\mathrm{N}(0,1)</script></span> 中选出 2000 组数据，1 组数据对应生成一个老虎机问题，每组数据有 10 个（所以其实是从 <span><span class="MathJax_Preview">\mathrm{N}(0,1)</span><script type="math/tex">\mathrm{N}(0,1)</script></span> 中选出了 <span><span class="MathJax_Preview">2000\times 10=20000</span><script type="math/tex">2000\times 10=20000</script></span> 个随机数），分别表示这个问题下的 <span><span class="MathJax_Preview">q_*(1),\ldots,q_*(10)</span><script type="math/tex">q_*(1),\ldots,q_*(10)</script></span> 。</li>
</ul>
<h3 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h3>
<div align="center"><img src="../imgs/RLAI_2/avg-reward.png" width="450" alt="avg-reward" /></div>

<p>从上图易知，</p>
<ul>
<li>整体上都能一定程度地通过学习找到问题的规律，所以三种策略最后都能有一个正的稳定回报值（如果没有学到任何信息，也即随机选 action ，最后这些 “<strong>average</strong> reward” 显然会趋于0。注意这个 “<strong>average</strong>” ，是指很多不同问题的平均）</li>
<li>贪心策略一开始的表现要比其他的略好，但是最终明显不如 ε-贪心策略（猜测是陷入了局部最优解）。</li>
</ul>
<div align="center"><img src="../imgs/RLAI_2/optimal-action.png" width="450" alt="optimal-action" /></div>

<p>从上图易知，</p>
<ul>
<li>贪心策略只有约 1/3 的次数选到了最优 action，而 ε-贪心策略的表现则显然比单纯的贪心策略好很多，进一步验证了我们认为贪心策略陷入局部最优解的猜想。</li>
<li>ε = 0.1 要比 ε = 0.01 选中最优解的概率更大，这与其重视 Exploration 离不开关系，但事实上实验结果表示，最终的总 reward 还是 ε = 0.01 要高一些，原因在于其有 99% 的时间处于 Exploitation 阶段，信息的利用率更高，ε = 0.1 时，Exploitation 的时间只有 90% ，即使探索到了足够多的信息，但是利用率不够高，导致最终效果不如前者。</li>
</ul>
<p>所以，通过实验，我们看出，Exploration 确实很重要，但是也不能过度探索，需要掌握好平衡。</p>
<h2 id="24-incremental-implementation">2.4 Incremental Implementation<a class="headerlink" href="#24-incremental-implementation" title="Permanent link">&para;</a></h2>
<p>这一节的简单讲提到如何让计算机来学习 bandit 问题。</p>
<h3 id="optimization">Optimization<a class="headerlink" href="#optimization" title="Permanent link">&para;</a></h3>
<p>首先，我们把问题简化一下，只关注某个具体的 action <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> ，其他的类比即可。</p>
<p>设 <span><span class="MathJax_Preview">R_i</span><script type="math/tex">R_i</script></span> 表示第 i 次选到 <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 时系统返回的奖励值，<span><span class="MathJax_Preview">Q_n</span><script type="math/tex">Q_n</script></span> 表示在前 n 次执行 action <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 的经验基础上，对下一次再选到 <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 的预测值，那么就有</p>
<div>
<div class="MathJax_Preview">Q_n\doteq \dfrac{R_1+R_2+\cdots + R_{n-1}}{n-1}</div>
<script type="math/tex; mode=display">Q_n\doteq \dfrac{R_1+R_2+\cdots + R_{n-1}}{n-1}</script>
</div>
<p>不难看出，我们一直需要存储每一个 <span><span class="MathJax_Preview">R_i</span><script type="math/tex">R_i</script></span> ，空间复杂度为 <span><span class="MathJax_Preview">O(n)</span><script type="math/tex">O(n)</script></span> ，这显然程序跑到后面，会有着巨大的内存占用。不过，通过一个小技巧便可解决：</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
Q_{n+1} &amp; = \dfrac{1}{n}\sum_{i=1}^{n}R_i = \dfrac{1}{n}\left(R_n+\sum_{i=1}^{n-1}R_i\right)\\
&amp; = \dfrac{1}{n}\left(R_n+(n-1)\dfrac{1}{n-1}\sum_{i=1}^{n-1}R_i\right)\\
&amp; = \dfrac{1}{n}\left(R_n+(n-1)Q_n\right) = \dfrac{1}{n}\left(R_n+nQ_n-Q_n\right)\\
&amp; = Q_n + \dfrac{1}{n}\left[R_n-Q_n\right],
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
Q_{n+1} & = \dfrac{1}{n}\sum_{i=1}^{n}R_i = \dfrac{1}{n}\left(R_n+\sum_{i=1}^{n-1}R_i\right)\\
& = \dfrac{1}{n}\left(R_n+(n-1)\dfrac{1}{n-1}\sum_{i=1}^{n-1}R_i\right)\\
& = \dfrac{1}{n}\left(R_n+(n-1)Q_n\right) = \dfrac{1}{n}\left(R_n+nQ_n-Q_n\right)\\
& = Q_n + \dfrac{1}{n}\left[R_n-Q_n\right],
\end{aligned}
</script>
</div>
<p>即 <span><span class="MathJax_Preview">Q_{n+1} = Q_n + \dfrac{1}{n}\left[R_n-Q_n\right]</span><script type="math/tex">Q_{n+1} = Q_n + \dfrac{1}{n}\left[R_n-Q_n\right]</script></span> ，如此一来，我们只需要存储 <span><span class="MathJax_Preview">Q, R, n</span><script type="math/tex">Q, R, n</script></span> ，每次覆写在变量上即可，空间复杂度降为 <span><span class="MathJax_Preview">O(1)</span><script type="math/tex">O(1)</script></span> ，计算量也有所下降。</p>
<p>上面式子的更广义的写法是</p>
<div>
<div class="MathJax_Preview">NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]</div>
<script type="math/tex; mode=display">NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]</script>
</div>
<h3 id="pseuducode">Pseuducode<a class="headerlink" href="#pseuducode" title="Permanent link">&para;</a></h3>
<p>在这个基础上，我们整理一下程序的流程，下面是程序的伪代码：</p>
<blockquote>
<p>Initialize, for a = 1 to k:</p>
<p><span><span class="MathJax_Preview">\qquad Q(a)\leftarrow 0</span><script type="math/tex">\qquad Q(a)\leftarrow 0</script></span></p>
<p><span><span class="MathJax_Preview">\qquad N(a)\leftarrow 0</span><script type="math/tex">\qquad N(a)\leftarrow 0</script></span></p>
<p>Repeat forever:</p>
<p><span><span class="MathJax_Preview">\qquad A \leftarrow \begin{cases} \mathop{\arg\max}\limits_{a}Q(a) &amp; \text{with probability} \ 1-\epsilon\\ \text{a random action} &amp; \text{with probability} \ \epsilon \end{cases}</span><script type="math/tex">\qquad A \leftarrow \begin{cases} \mathop{\arg\max}\limits_{a}Q(a) & \text{with probability} \ 1-\epsilon\\ \text{a random action} & \text{with probability} \ \epsilon \end{cases}</script></span></p>
<p><span><span class="MathJax_Preview">\qquad R\leftarrow bandit(A)</span><script type="math/tex">\qquad R\leftarrow bandit(A)</script></span></p>
<p><span><span class="MathJax_Preview">\qquad N(A)\leftarrow N(A)+1</span><script type="math/tex">\qquad N(A)\leftarrow N(A)+1</script></span></p>
<p><span><span class="MathJax_Preview">\qquad Q(A)\leftarrow Q(A) + \dfrac{1}{N(A)}[R-Q(A)]</span><script type="math/tex">\qquad Q(A)\leftarrow Q(A) + \dfrac{1}{N(A)}[R-Q(A)]</script></span></p>
</blockquote>
<h2 id="25-tracking-a-nonstationary-problem">2.5 Tracking a Nonstationary Problem<a class="headerlink" href="#25-tracking-a-nonstationary-problem" title="Permanent link">&para;</a></h2>
<p>前面的讨论，都是基于<strong>固定奖励值分布</strong>这一条件的，即 <span><span class="MathJax_Preview">R_t\sim \mathrm{N}(q_*(A_t),1)</span><script type="math/tex">R_t\sim \mathrm{N}(q_*(A_t),1)</script></span> 这一事实在问题生成好之后都是一直保持不变的。然而现实中问题肯定不会如此理想，那么如果这个奖励值分布不是固定不变的，我们该如何解决呢？</p>
<p>显然，这种情况下玩家需要将学习的重心放在每一步近期的奖励值分布情况上，这是因为，奖励值分布的变动，如果有规律的话，无论是周期性还是连续性等，都只会更多地体现在较近时刻，而很久之前的某个 reward 对于这一步而言已经很难看出其影响意义。</p>
<p>一个常见的作法是，将前面提到的增量式中的参数 StepSize 设为一个常量 <span><span class="MathJax_Preview">\alpha \in (0,1]</span><script type="math/tex">\alpha \in (0,1]</script></span> ，则有</p>
<div>
<div class="MathJax_Preview">Q_{n+1}\doteq Q_n + \alpha[R_n-Q_n] (\alpha \in (0,1])</div>
<script type="math/tex; mode=display">Q_{n+1}\doteq Q_n + \alpha[R_n-Q_n] (\alpha \in (0,1])</script>
</div>
<p>（再强调一遍，这几节的很多式子都是为了简化而针对某一个 action 而言的，可以看作是所有 action 的通式，不要理解错了）</p>
<p>我们不断对此式作展开，</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
Q_{n+1} &amp;\doteq Q_{n} + \alpha[R_{n} - Q_{n}]
\\ &amp;= \alpha R_{n} + (1 - \alpha)Q_{n}
\\ &amp;= \alpha R_{n} + (1 - \alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}]
\\ &amp;= \alpha R_{n} + (1 -\alpha)\alpha R_{n-1} + (1 - \alpha)^{2}Q_{n-1}
\\ &amp;= \alpha R_{n} + (1 - \alpha)\alpha R_{n-1} + (1-\alpha)^{2}\alpha R_{n-2} +...
\\ &amp;+(1-\alpha)^{n-1}\alpha R_{1} + (1-\alpha)^{n}Q_{1}
\\ &amp;= (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
Q_{n+1} &\doteq Q_{n} + \alpha[R_{n} - Q_{n}]
\\ &= \alpha R_{n} + (1 - \alpha)Q_{n}
\\ &= \alpha R_{n} + (1 - \alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}]
\\ &= \alpha R_{n} + (1 -\alpha)\alpha R_{n-1} + (1 - \alpha)^{2}Q_{n-1}
\\ &= \alpha R_{n} + (1 - \alpha)\alpha R_{n-1} + (1-\alpha)^{2}\alpha R_{n-2} +...
\\ &+(1-\alpha)^{n-1}\alpha R_{1} + (1-\alpha)^{n}Q_{1}
\\ &= (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}
\end{aligned}
</script>
</div>
<p>最终整理得到</p>
<div>
<div class="MathJax_Preview">Q_{n+1} = (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}</div>
<script type="math/tex; mode=display">Q_{n+1} = (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}</script>
</div>
<p>因为 <span><span class="MathJax_Preview">\displaystyle (1-\alpha)^{n}+\sum^{n}_{i=1}\alpha(1-\alpha)^{n-i}=1</span><script type="math/tex">\displaystyle (1-\alpha)^{n}+\sum^{n}_{i=1}\alpha(1-\alpha)^{n-i}=1</script></span> ，因此这是一个加权平均式，作者将此式称为<strong>指数近因加权平均（Exponential Recency-weighted Average）</strong>。</p>
<p>可以看出，当 i 很大时，<span><span class="MathJax_Preview">R_i</span><script type="math/tex">R_i</script></span> 在式子中的影响占比才更大，这也符合了我们要将学习重心放在近期 reward 的要求。下面再讲将 <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 设为常量的另一个重要原因。</p>
<p>我们先回到一般情况，对于 <span><span class="MathJax_Preview">Q_{n+1} = Q_{n} + \alpha_n[R_{n} - Q_{n}]</span><script type="math/tex">Q_{n+1} = Q_{n} + \alpha_n[R_{n} - Q_{n}]</script></span> ，其中的 <span><span class="MathJax_Preview">\alpha_n</span><script type="math/tex">\alpha_n</script></span> 是任意的，也即 step-size 是变长的，对于这样一组 <span><span class="MathJax_Preview">\{\alpha_n\}</span><script type="math/tex">\{\alpha_n\}</script></span> 序列，如果满足随机逼近理论中的一个条件</p>
<div>
<div class="MathJax_Preview">\sum_{n=1}^{\infty}\alpha_n(a)=\infty\quad \text{and}\quad \sum_{n=1}^{\infty}\alpha_n^2(a)&lt;\infty</div>
<script type="math/tex; mode=display">\sum_{n=1}^{\infty}\alpha_n(a)=\infty\quad \text{and}\quad \sum_{n=1}^{\infty}\alpha_n^2(a)<\infty</script>
</div>
<p>那么 <span><span class="MathJax_Preview">Q_n</span><script type="math/tex">Q_n</script></span> 将会以概率 1 收敛。</p>
<p>我们只简单定性分析一下这两个条件的意义：</p>
<ul>
<li>
<p><span><span class="MathJax_Preview">\displaystyle \sum_{n=1}^{\infty}\alpha_n(a)=\infty</span><script type="math/tex">\displaystyle \sum_{n=1}^{\infty}\alpha_n(a)=\infty</script></span> 能够确保总的步数足够长，进而摆脱初始条件和随机波动的影响。</p>
</li>
<li>
<p><span><span class="MathJax_Preview">\displaystyle \sum_{n=1}^{\infty}\alpha_n^2(a)&lt;\infty</span><script type="math/tex">\displaystyle \sum_{n=1}^{\infty}\alpha_n^2(a)<\infty</script></span> 确保最终的步长足够小，进而能够收敛。</p>
</li>
</ul>
<p>易见，<span><span class="MathJax_Preview">\alpha_n = \dfrac{1}{n}</span><script type="math/tex">\alpha_n = \dfrac{1}{n}</script></span> 满足条件能够让其收敛，而 <span><span class="MathJax_Preview">\alpha_n \equiv \alpha</span><script type="math/tex">\alpha_n \equiv \alpha</script></span>(<span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 为常量) 则由于不满足条件中的第二项，使 <span><span class="MathJax_Preview">Q_n</span><script type="math/tex">Q_n</script></span> 不能收敛。看似这是一个坏结果，其实这一结果反而能被利用在<strong>非稳定（nonstationary）问题</strong>中，这是因为，一个不收敛的波动的 <span><span class="MathJax_Preview">Q_n</span><script type="math/tex">Q_n</script></span> 其实更适合用来描述非稳定问题下的奖励值，而前面收敛的 <span><span class="MathJax_Preview">Q_n</span><script type="math/tex">Q_n</script></span> 反而可能失去了非稳定环境下的一些关键波动信息。</p>
<p>另一个关键之处在于，满足两个条件的 <span><span class="MathJax_Preview">\{\alpha_n\}</span><script type="math/tex">\{\alpha_n\}</script></span> 往往收敛缓慢，非常不实用，一般也只用在理论研究中。</p>
<h2 id="26-optimistic-initial-values">2.6 Optimistic Initial Values<a class="headerlink" href="#26-optimistic-initial-values" title="Permanent link">&para;</a></h2>
<p>这一节简单研究了一下初始预估值对模型学习效果的影响。</p>
<p>我们再次拿出前面的指数近因加权平均：</p>
<div>
<div class="MathJax_Preview">Q_{n+1} = (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}</div>
<script type="math/tex; mode=display">Q_{n+1} = (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}</script>
</div>
<p>不难看出，前面讨论的所有方法，对每个 action 而言，评估体系显然都会一定程度上受到初始值 <span><span class="MathJax_Preview">Q_1</span><script type="math/tex">Q_1</script></span> 的影响。在统计学中，这叫做被初值<em>偏置</em>了。</p>
<p>初始预估值可以用来根据先验信息提供奖励的期望标准。此外，如果将初始值调高，还有着鼓励模型在早期更多地进行探索的作用。以贪心策略为例，一个很高的初始预期值（称为<strong>乐观初值</strong>），会诱使模型去选择这个 action ，然而事实上 reward 要比估计值差很多，误差值 <span><span class="MathJax_Preview">[R_n - Q_n]</span><script type="math/tex">[R_n - Q_n]</script></span> 会是一个较大的负数，导致模型对这个 action “失望”，评价降低，下一次，模型就会去主动尝试其他 action 。通过这一方法，达到了鼓励模型在早期多做探索的作用。</p>
<p>下面是一个具体的实验，“高初始值的 greedy 策略” vs “正常初始值的 ε-greedy 策略”。</p>
<div align="center"><img src="../imgs/RLAI_2/optimistic-init.png" width="450" alt="optimistic-init" /></div>

<p>从图片可以看出，即使是纯贪心行动，由于一开始给定较高初始值，模型便如我们分析的一样，在早期进行了大量探索，收获了大量有用的信息，从而也能摆脱局部最优，达到全局最优解，而且其后期几乎 100% 利用率的优势，使其比该实验中 ε-greedy 方法的效果还要优秀。</p>
<p>但是乐观初值法的适用面很窄，它仅适用于固定分布的问题。我们知道模型只会在早期多做探索，后期基本上仍是以 Exploiting 为主，对于非稳定的情况，必然需要时刻探索收集信息，此时乐观初值法就不再适用。</p>
<h2 id="27-upper-confidence-bound-action-selection">2.7 Upper-Confidence-Bound Action Selection<a class="headerlink" href="#27-upper-confidence-bound-action-selection" title="Permanent link">&para;</a></h2>
<p>这一节讲到一个考虑得更全面的评估算法：<strong>Upper-Confidence-Bound(UCB)</strong> 算法。简单讲，就是我们之前的 ε-greedy 方法虽然能保证最终能探索到足够的信息，但是效率不高，因为他只是简单的随机探索，探索时每个 action 都是等概率被选择的。</p>
<p>我们可以想一想人是怎样探索学习的。人在探索过程中，通过探索学到的知识，肯定会建立一套标准来判定好坏，如果重复执行某个 action ，一直返回一个低回报，那么必须要动态调整探索策略，适当调低再探索这个 action 的概率，而要尽可能多去探索“潜力”更高的 action 。UCB 算法做的就是这么一件事。</p>
<p>那么 UCB 算法具体是什么呢？</p>
<div>
<div class="MathJax_Preview">A_{t} \doteq \mathop{\arg\max}_{a}\left[Q_{t}(a) + c\sqrt{\frac{\ln{t}}{N_{t}(a)}}\right]</div>
<script type="math/tex; mode=display">A_{t} \doteq \mathop{\arg\max}_{a}\left[Q_{t}(a) + c\sqrt{\frac{\ln{t}}{N_{t}(a)}}\right]</script>
</div>
<p>UCB 算法就是采取满足上式的 action <span><span class="MathJax_Preview">A_t</span><script type="math/tex">A_t</script></span> ，算法的核心就在于我们新加入的 <span><span class="MathJax_Preview">c\sqrt{\dfrac{\ln t}{N_t(a)}}</span><script type="math/tex">c\sqrt{\dfrac{\ln t}{N_t(a)}}</script></span> 。</p>
<ul>
<li><span><span class="MathJax_Preview">c\sqrt{\dfrac{\ln t}{N_t(a)}}</span><script type="math/tex">c\sqrt{\dfrac{\ln t}{N_t(a)}}</script></span> ：对于估值的不确定性。更广义地讲，其意义为方差。</li>
<li><span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> ：控制了探索的程度，决定了置信度。</li>
<li><span><span class="MathJax_Preview">N_t(a)</span><script type="math/tex">N_t(a)</script></span> ：第 t 步之前 action <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 被选中的次数。<span><span class="MathJax_Preview">N_t(a)</span><script type="math/tex">N_t(a)</script></span> 如果增加，会给公式中的此项带来降低的影响效果。</li>
<li><span><span class="MathJax_Preview">\ln t</span><script type="math/tex">\ln t</script></span> ：时间必然会增加，故此项也是一直在保持增加的。但他的影响效果与 action <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 的选择状态密切相关。如果 <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 不被选择， <span><span class="MathJax_Preview">\ln t</span><script type="math/tex">\ln t</script></span> 增加但 <span><span class="MathJax_Preview">N_t(a)</span><script type="math/tex">N_t(a)</script></span> 不变，故此项变大，不确定性增加；反之，<span><span class="MathJax_Preview">N_{t+1}(a) = N_t(a) + 1</span><script type="math/tex">N_{t+1}(a) = N_t(a) + 1</script></span> ，虽然 <span><span class="MathJax_Preview">\ln t</span><script type="math/tex">\ln t</script></span> 增加但其增速不如 <span><span class="MathJax_Preview">N_t(a)</span><script type="math/tex">N_t(a)</script></span> ，所以此项整体变小，不确定性降低。</li>
</ul>
<p>所以，通过取 <span><span class="MathJax_Preview">\mathop{\arg\max}</span><script type="math/tex">\mathop{\arg\max}</script></span> 便能动态调整探索策略，适当地提高更加不确定、有潜力的 action 被探索的概率。</p>
<div align="center"><img src="../imgs/RLAI_2/ucb.png" width="450" alt="ucb" /></div>

<p>通过对比实验发现，UCB 算法确实表现要优于普通的 ε-greedy 算法。但是作者也提到，对于更一般的强化学习问题，UCB 算法会遇到一些难点，不再那么适用，比如非稳定问题、大状态空间问题等。</p>
<h2 id="28-gradient-bandit-algorithms">2.8 Gradient Bandit Algorithms<a class="headerlink" href="#28-gradient-bandit-algorithms" title="Permanent link">&para;</a></h2>
<h3 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h3>
<p>前面几种算法，都是在围绕着 <span><span class="MathJax_Preview">Q_t</span><script type="math/tex">Q_t</script></span> 进行取 <span><span class="MathJax_Preview">\mathop{\arg\max}</span><script type="math/tex">\mathop{\arg\max}</script></span> 然后直接执行 action 的策略，显得有点偏激，一个看上去更合理的做法是，每个 action 对其评分后，确定一个概率分布，然后以这种分布下的<strong>趋势</strong>去做选择，而非凭借数值的绝对大小去做选择。这样显得更加“平滑”，同时根据这些趋势，也能达到动态探索的效果。这便是 Gradient Bandit Algorithms 。</p>
<div>
<div class="MathJax_Preview">\mathrm{Pr}\{A_{t} = a\} \doteq \frac{e^{H_{t}(a)}}{\sum^{k}_{b=1}e^{H_{t}(b)}}\doteq \pi_{t}(a)</div>
<script type="math/tex; mode=display">\mathrm{Pr}\{A_{t} = a\} \doteq \frac{e^{H_{t}(a)}}{\sum^{k}_{b=1}e^{H_{t}(b)}}\doteq \pi_{t}(a)</script>
</div>
<p>其中，<span><span class="MathJax_Preview">H_t(a)</span><script type="math/tex">H_t(a)</script></span> 是 action <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 的偏好值，也就是前面提到的对每个 action 的评分，然后根据 <em>soft-max</em> 函数来给出选择每个 action 的概率分布。</p>
<p>而在取得每步的反馈后，我们则需要利用随机梯度上升法来更新偏好值</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
H_{t+1}(A_{t})&amp;\doteq H_{t}(A_{t}) + \alpha(R_{t}-\overline R_{t})(1-\pi_{t}(A_{t}))
\\H_{t+1}(a) &amp;\doteq H_{t}(a) - \alpha(R_{t} - \overline R_{t})\pi_{t}(a),\ \forall a \neq A_{t}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
H_{t+1}(A_{t})&\doteq H_{t}(A_{t}) + \alpha(R_{t}-\overline R_{t})(1-\pi_{t}(A_{t}))
\\H_{t+1}(a) &\doteq H_{t}(a) - \alpha(R_{t} - \overline R_{t})\pi_{t}(a),\ \forall a \neq A_{t}
\end{aligned}
</script>
</div>
<p>更一般地，我们可以用指示函数来写成一个通式</p>
<div>
<div class="MathJax_Preview">H_{t+1}(a) \doteq H_{t}(a) + \alpha(R_{t}-\overline R_{t})(\textbf{1}_{a = A_t}-\pi_{t}(A_{t}))</div>
<script type="math/tex; mode=display">H_{t+1}(a) \doteq H_{t}(a) + \alpha(R_{t}-\overline R_{t})(\textbf{1}_{a = A_t}-\pi_{t}(A_{t}))</script>
</div>
<h3 id="proof">Proof<a class="headerlink" href="#proof" title="Permanent link">&para;</a></h3>
<p>我们知道，随机梯度上升确实能确保收敛到最优值，那么问题就在于，这个形式是否就是“随机梯度上升”的形式呢？</p>
<p>结论先摆出来，上面的方法确实满足随机梯度上升的条件。证明如下：</p>
<p>只需证明</p>
<div>
<div class="MathJax_Preview">
H_{t+1}(a) \doteq H_{t}(a) + \alpha \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}
</div>
<script type="math/tex; mode=display">
H_{t+1}(a) \doteq H_{t}(a) + \alpha \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}
</script>
</div>
<p>其中</p>
<div>
<div class="MathJax_Preview">
\mathbb{E}[R_t] \doteq \sum_{b} \pi_t(b)q_*(b)
</div>
<script type="math/tex; mode=display">
\mathbb{E}[R_t] \doteq \sum_{b} \pi_t(b)q_*(b)
</script>
</div>
<p>而</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}&amp;=\frac{\partial}{\partial H_t(a)}\left[\sum_{b}\pi_t(b)q_*(b)\right]
\\&amp;=\sum_b q_*(b)\frac{\partial \pi_t(b)}{\partial H_t(a)}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}&=\frac{\partial}{\partial H_t(a)}\left[\sum_{b}\pi_t(b)q_*(b)\right]
\\&=\sum_b q_*(b)\frac{\partial \pi_t(b)}{\partial H_t(a)}
\end{aligned}
</script>
</div>
<p>任设一个标量 <span><span class="MathJax_Preview">X_t</span><script type="math/tex">X_t</script></span> 与 <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 独立，而显然又有 <span><span class="MathJax_Preview">\displaystyle \sum_b \frac{\partial \pi_t(b)}{\partial H_t(a)} = 0</span><script type="math/tex">\displaystyle \sum_b \frac{\partial \pi_t(b)}{\partial H_t(a)} = 0</script></span> ，因此 <span><span class="MathJax_Preview">\displaystyle X_t\sum_b \frac{\partial \pi_t(b)}{\partial H_t(a)} = 0</span><script type="math/tex">\displaystyle X_t\sum_b \frac{\partial \pi_t(b)}{\partial H_t(a)} = 0</script></span> 。接上式，</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
\\&amp;=\sum_b(q_*(b) - X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}
\\&amp;=\sum_b \pi_t(b)(q_*(b) - X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}/\pi_t(b)
\\&amp;=\mathbb{E}[(q_*(A_t) - X_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\\&=\sum_b(q_*(b) - X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}
\\&=\sum_b \pi_t(b)(q_*(b) - X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}/\pi_t(b)
\\&=\mathbb{E}[(q_*(A_t) - X_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
\end{aligned}
</script>
</div>
<p>在给定 <span><span class="MathJax_Preview">A_t</span><script type="math/tex">A_t</script></span> 的条件下， <span><span class="MathJax_Preview">\mathbb{E}[R_t|A_t]=q_*(A_t)</span><script type="math/tex">\mathbb{E}[R_t|A_t]=q_*(A_t)</script></span> ， <span><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> 又与其他项不相关，故对于上式，期望意义下可以把 <span><span class="MathJax_Preview">q_*(A_t)</span><script type="math/tex">q_*(A_t)</script></span> 替换为 <span><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> ，此时再将任设的 <span><span class="MathJax_Preview">X_t</span><script type="math/tex">X_t</script></span> 定为 <span><span class="MathJax_Preview">R_t</span><script type="math/tex">R_t</script></span> 的均值 <span><span class="MathJax_Preview">\overline R_t</span><script type="math/tex">\overline R_t</script></span> ，则有</p>
<div>
<div class="MathJax_Preview">
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}=\mathbb{E}[(R_t - \overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
</div>
<script type="math/tex; mode=display">
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}=\mathbb{E}[(R_t - \overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
</script>
</div>
<p>由于</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
\frac{\partial \pi_t(b)}{\partial H_t(a)} &amp;= \frac{\partial}{\partial H_t(a)}\pi_t(b)
\\&amp;= \frac{\partial}{\partial H_t(a)}[\frac{e^{H_{t}(b)}}{\sum^{k}_{c=1}e^{H_{t}(c)}}]
\\&amp;= \frac{\frac{\partial e^{H_t(b)}}{\partial H_t(a)}\sum_{c=1}^k e^{H_t(c)}-e^{H_t(b)} \frac{\partial \sum_{c=1}^k e^{H_t(c)}}{\partial H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&amp;=\frac{\textbf{1}_{a = b}e^{H_t(a)}\sum_{c=1}^k e^H_t(c) - e^{H_t(b)}e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&amp;=\frac{\textbf{1}_{a = b}e^{H_t(b)}}{\sum_{c=1}^k e^{H_t(c)}} - \frac{e^{H_t(b)}e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&amp;=\textbf{1}_{a=b}\pi_t(b)-\pi_t(b)\pi_t(a)
\\&amp;=\pi_t(b)(\textbf{1}_{a=b}-\pi_t(a))
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \pi_t(b)}{\partial H_t(a)} &= \frac{\partial}{\partial H_t(a)}\pi_t(b)
\\&= \frac{\partial}{\partial H_t(a)}[\frac{e^{H_{t}(b)}}{\sum^{k}_{c=1}e^{H_{t}(c)}}]
\\&= \frac{\frac{\partial e^{H_t(b)}}{\partial H_t(a)}\sum_{c=1}^k e^{H_t(c)}-e^{H_t(b)} \frac{\partial \sum_{c=1}^k e^{H_t(c)}}{\partial H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&=\frac{\textbf{1}_{a = b}e^{H_t(a)}\sum_{c=1}^k e^H_t(c) - e^{H_t(b)}e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&=\frac{\textbf{1}_{a = b}e^{H_t(b)}}{\sum_{c=1}^k e^{H_t(c)}} - \frac{e^{H_t(b)}e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&=\textbf{1}_{a=b}\pi_t(b)-\pi_t(b)\pi_t(a)
\\&=\pi_t(b)(\textbf{1}_{a=b}-\pi_t(a))
\end{aligned}
</script>
</div>
<p>代回前面的式子得到</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}&amp;=\mathbb{E}[(R_t - \overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
\\&amp;=\mathbb{E}[(R_t - \overline R_t)\pi_t(A_t)(\textbf{1}_{a = A_t} - \pi_t(a))/\pi_t(A_t)]
\\&amp;=\mathbb{E}[(R_t - \overline R_t)(\textbf{1}_{a = A_t} - \pi_t(a))]
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}&=\mathbb{E}[(R_t - \overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
\\&=\mathbb{E}[(R_t - \overline R_t)\pi_t(A_t)(\textbf{1}_{a = A_t} - \pi_t(a))/\pi_t(A_t)]
\\&=\mathbb{E}[(R_t - \overline R_t)(\textbf{1}_{a = A_t} - \pi_t(a))]
\end{aligned}
</script>
</div>
<p>结合上面的结果，此时再来对比我们的两个目标式</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
H_{t+1}(a) &amp;= H_{t}(a) + \alpha \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}
\\H_{t+1}(a) &amp;= H_{t}(a) + \alpha(R_{t}-\overline R_{t})(\textbf{1}_{a = A_t}-\pi_{t}(A_{t}))
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
H_{t+1}(a) &= H_{t}(a) + \alpha \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}
\\H_{t+1}(a) &= H_{t}(a) + \alpha(R_{t}-\overline R_{t})(\textbf{1}_{a = A_t}-\pi_{t}(A_{t}))
\end{aligned}
</script>
</div>
<p>发现确实是梯度上升的形式，证明完毕。</p>
<div align="center"><img src="../imgs/RLAI_2/sga.png" width="450" alt="sga" /></div>

<p>关于 <span><span class="MathJax_Preview">H_t(a)</span><script type="math/tex">H_t(a)</script></span> 的更新式中 <span><span class="MathJax_Preview">\overline R_{t}</span><script type="math/tex">\overline R_{t}</script></span> 这一项，他起到一个对比基准线的作用，事实上这个基准线不一定设为均值，他的取值并不影响更新式的方差。作者表明，其实设为均值并不一定能达到最佳效果，但总体而言是一个简单方便且效果较好的一个选择。上图中的实验简单对比了 baseline 为均值和 baseline 为 0 时的不同效果。</p>
<h2 id="29-associative-search-contextual-bandit">2.9 Associative Search (Contextual Bandit)<a class="headerlink" href="#29-associative-search-contextual-bandit" title="Permanent link">&para;</a></h2>
<p>本文的一开头，我们提到本章主要针对“非关联性（nonassociative）”的简单场景来学习基础的强化学习方法。而非关联性在本章就是指<strong>无需考虑每一步行动之间的影响，以及环境对行动的影响</strong>。非关联性问题是一种很理想化的问题，现实中很多东西都是有所联系的，包括 action 与 action 之间的关联， action 与环境之间的关联等等。这一小节，就是关于关联性问题做了一个最基本的简单介绍。</p>
<h3 id="background_2">Background<a class="headerlink" href="#background_2" title="Permanent link">&para;</a></h3>
<ul>
<li>考虑有 m 个独立的 <span><span class="MathJax_Preview">k_i</span><script type="math/tex">k_i</script></span>-armed bandit 任务（<span><span class="MathJax_Preview">i=1,\ldots,m</span><script type="math/tex">i=1,\ldots,m</script></span>），每个都有独特的特征能被区分开。</li>
<li>每一步会让你面对一个 <span><span class="MathJax_Preview">k_i</span><script type="math/tex">k_i</script></span>-armed bandit 任务来做选择。</li>
<li>目标是学习出能将这 m 个独立任务关联起来的最优方案。</li>
</ul>
<h3 id="full-reinforcement-learning-problem">Full Reinforcement Learning Problem<a class="headerlink" href="#full-reinforcement-learning-problem" title="Permanent link">&para;</a></h3>
<p>简单而言，之前一直讨论的 nonassociative 问题可以看作现在这个问题下 m=1 的特例。在这个新任务中，我们不但要像之前一样通过探索和利用来学习每个问题的情况，还要把问题之间的关联性也学出来，也就是把环境因素也考虑进来。</p>
<p>这种复杂的问题，叫做 full reinforcement learning problem ，会在书的后面章节讲到。</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../../" title="README" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                README
              </span>
            </div>
          </a>
        
        
          <a href="../../../../projects/" title="README" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                README
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2016-2019 ONCE.MATH
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/zawnpn" class="md-footer-social__link fa fa-github"></a>
    
      <a href="https://twitter.com/zawnpn" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="https://steamcommunity.com/id/zawnpn/" class="md-footer-social__link fa fa-steam"></a>
    
      <a href="https://www.zhihu.com/people/zhangwanpeng" class="md-footer-social__link fa fa-globe"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../../assets/javascripts/application.583bbe55.js"></script>
      
        
        
          
          <script src="../../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
          
            
              
                <script src="../../../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../../../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
            <script src="../../../../assets/javascripts/lunr/lunr.multi.js"></script>
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../../.."}})</script>
      
        <script src="../../../../assets/extra.js"></script>
      
        <script src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=?config=TeX-MML-AM_SVG"></script>
      
    
    
      
    
  </body>
</html>
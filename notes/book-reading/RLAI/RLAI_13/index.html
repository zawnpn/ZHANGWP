<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to zhangwp's blog."><meta name=author content="Wanpeng Zhang"><link href=https://www.zhangwp.com/notes/book-reading/RLAI/RLAI_13/ rel=canonical><link href=../RLAI_12/ rel=prev><link href=../../../../share/ rel=next><link rel=icon href=../../../../_static/images/favicon.svg><meta name=generator content="mkdocs-1.4.2, mkdocs-material-9.1.6"><title>Chapter 13 - ZHANGWP</title><link rel=stylesheet href=../../../../assets/stylesheets/main.ded33207.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.a0c5b2b5.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Noto Sans SC";--md-code-font:"Fira Code"}</style><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=light-blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#- class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title=ZHANGWP class="md-header__button md-logo" aria-label=ZHANGWP data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256 9.4 86.6zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> ZHANGWP </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Chapter 13 </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/zawnpn/ZHANGWP title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M80 104a24 24 0 1 0 0-48 24 24 0 1 0 0 48zm80-24c0 32.8-19.7 61-48 73.3v87.8c18.8-10.9 40.7-17.1 64-17.1h96c35.3 0 64-28.7 64-64v-6.7c-28.3-12.3-48-40.5-48-73.3 0-44.2 35.8-80 80-80s80 35.8 80 80c0 32.8-19.7 61-48 73.3v6.7c0 70.7-57.3 128-128 128h-96c-35.3 0-64 28.7-64 64v6.7c28.3 12.3 48 40.5 48 73.3 0 44.2-35.8 80-80 80S0 476.2 0 432c0-32.8 19.7-61 48-73.3V153.3C19.7 141 0 112.8 0 80 0 35.8 35.8 0 80 0s80 35.8 80 80zm232 0a24 24 0 1 0-48 0 24 24 0 1 0 48 0zM80 456a24 24 0 1 0 0-48 24 24 0 1 0 0 48z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../ class="md-tabs__link md-tabs__link--active"> Note </a> </li> <li class=md-tabs__item> <a href=../../../../share/ class=md-tabs__link> Share </a> </li> <li class=md-tabs__item> <a href=../../../../other/ class=md-tabs__link> Other </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title=ZHANGWP class="md-nav__button md-logo" aria-label=ZHANGWP data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256 9.4 86.6zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32z"/></svg> </a> ZHANGWP </label> <div class=md-nav__source> <a href=https://github.com/zawnpn/ZHANGWP title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M80 104a24 24 0 1 0 0-48 24 24 0 1 0 0 48zm80-24c0 32.8-19.7 61-48 73.3v87.8c18.8-10.9 40.7-17.1 64-17.1h96c35.3 0 64-28.7 64-64v-6.7c-28.3-12.3-48-40.5-48-73.3 0-44.2 35.8-80 80-80s80 35.8 80 80c0 32.8-19.7 61-48 73.3v6.7c0 70.7-57.3 128-128 128h-96c-35.3 0-64 28.7-64 64v6.7c28.3 12.3 48 40.5 48 73.3 0 44.2-35.8 80-80 80S0 476.2 0 432c0-32.8 19.7-61 48-73.3V153.3C19.7 141 0 112.8 0 80 0 35.8 35.8 0 80 0s80 35.8 80 80zm232 0a24 24 0 1 0-48 0 24 24 0 1 0 48 0zM80 456a24 24 0 1 0 0-48 24 24 0 1 0 0 48z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> Home <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. class=md-nav__link> Home </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> Note <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Note </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> Paper Reading <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Paper Reading </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1> <label class=md-nav__link for=__nav_2_2_1 id=__nav_2_2_1_label tabindex=0> Model-based RL <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1> <span class="md-nav__icon md-icon"></span> Model-based RL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../paper-reading/MBRL_with_uncertainty/ class=md-nav__link> MBRL with uncertainty </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_2> <label class=md-nav__link for=__nav_2_2_2 id=__nav_2_2_2_label tabindex=0> RL Control <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_2> <span class="md-nav__icon md-icon"></span> RL Control </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../paper-reading/MCTS_introduction/ class=md-nav__link> MCTS introduction </a> </li> <li class=md-nav__item> <a href=../../../paper-reading/background_and_decision-time_planning/ class=md-nav__link> Background and decision-time planning </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3 checked> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> Book Reading <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=true> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Book Reading </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1 checked> <label class=md-nav__link for=__nav_2_3_1 id=__nav_2_3_1_label tabindex=0> Reinforcement Learning An Introduction <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_3_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_3_1> <span class="md-nav__icon md-icon"></span> Reinforcement Learning An Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../RLAI_2/ class=md-nav__link> Chapter 2 </a> </li> <li class=md-nav__item> <a href=../RLAI_3/ class=md-nav__link> Chapter 3 </a> </li> <li class=md-nav__item> <a href=../RLAI_4/ class=md-nav__link> Chapter 4 </a> </li> <li class=md-nav__item> <a href=../RLAI_5/ class=md-nav__link> Chapter 5 </a> </li> <li class=md-nav__item> <a href=../RLAI_6/ class=md-nav__link> Chapter 6 </a> </li> <li class=md-nav__item> <a href=../RLAI_7/ class=md-nav__link> Chapter 7 </a> </li> <li class=md-nav__item> <a href=../RLAI_8/ class=md-nav__link> Chapter 8 </a> </li> <li class=md-nav__item> <a href=../RLAI_9/ class=md-nav__link> Chapter 9 </a> </li> <li class=md-nav__item> <a href=../RLAI_10/ class=md-nav__link> Chapter 10 </a> </li> <li class=md-nav__item> <a href=../RLAI_11/ class=md-nav__link> Chapter 11 </a> </li> <li class=md-nav__item> <a href=../RLAI_12/ class=md-nav__link> Chapter 12 </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Chapter 13 <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Chapter 13 </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#131-policy-approximation-and-its-advantages class=md-nav__link> 13.1 Policy Approximation and its Advantages </a> </li> <li class=md-nav__item> <a href=#132-the-policy-gradient-theorem class=md-nav__link> 13.2 The Policy Gradient Theorem </a> </li> <li class=md-nav__item> <a href=#133-reinforce-monte-carlo-policy-gradient class=md-nav__link> 13.3 REINFORCE: Monte Carlo Policy Gradient </a> </li> <li class=md-nav__item> <a href=#134-reinforce-with-baseline class=md-nav__link> 13.4 REINFORCE with Baseline </a> </li> <li class=md-nav__item> <a href=#135-actorcritic-methods class=md-nav__link> 13.5 Actor–Critic Methods </a> </li> <li class=md-nav__item> <a href=#136-policy-gradient-for-continuing-problems class=md-nav__link> 13.6 Policy Gradient for Continuing Problems </a> </li> <li class=md-nav__item> <a href=#137-policy-parameterization-for-continuous-actions class=md-nav__link> 13.7 Policy Parameterization for Continuous Actions </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> Share <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Share </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex=0> Projects <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Projects </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2_1> <label class=md-nav__link for=__nav_3_2_1 id=__nav_3_2_1_label tabindex=0> AI <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2_1> <span class="md-nav__icon md-icon"></span> AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/projects/rl_runfast/ class=md-nav__link> RL Runfast </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2_2> <label class=md-nav__link for=__nav_3_2_2 id=__nav_3_2_2_label tabindex=0> Typesetting <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2_2> <span class="md-nav__icon md-icon"></span> Typesetting </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/projects/markdown-toolkit/ class=md-nav__link> Markdown 编译转换工具 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2_3> <label class=md-nav__link for=__nav_3_2_3 id=__nav_3_2_3_label tabindex=0> NKU <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2_3> <span class="md-nav__icon md-icon"></span> NKU </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/projects/nku-eamis/ class=md-nav__link> NKU-EAMIS工具 </a> </li> <li class=md-nav__item> <a href=../../../../share/projects/nku-sms-rss/ class=md-nav__link> NKU-SMS-RSS </a> </li> <li class=md-nav__item> <a href=../../../../share/projects/eamis-miniapp/ class=md-nav__link> NKU-EAMIS_MiniApp(南开大学教务助手小程序) </a> </li> <li class=md-nav__item> <a href=../../../../share/projects/eamis-workflow/ class=md-nav__link> NKU-EAMIS for iOS(Workflow) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2_4> <label class=md-nav__link for=__nav_3_2_4 id=__nav_3_2_4_label tabindex=0> Game <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2_4> <span class="md-nav__icon md-icon"></span> Game </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/projects/steam-market-price-bot/ class=md-nav__link> Steam市场比价爬虫 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex=0> Diary <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Diary </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/roc-fly/ class=md-nav__link> 鹏程万里 </a> </li> <li class=md-nav__item> <a href=../../../../share/blog-history/ class=md-nav__link> 博客历史 </a> </li> <li class=md-nav__item> <a href=../../../../share/game-log/ class=md-nav__link> Game-Log </a> </li> <li class=md-nav__item> <a href=../../../../share/my-postgraduate-share/ class=md-nav__link> 保研推免经验分享 - 数学系跨保 CS </a> </li> <li class=md-nav__item> <a href=../../../../share/github-student-pack/ class=md-nav__link> Student Developer Pack - GitHub Education </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4 id=__nav_3_4_label tabindex=0> 数学建模 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> 数学建模 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/2017-mcm-icm/ class=md-nav__link> 2017美赛参赛整理(Problem D) </a> </li> <li class=md-nav__item> <a href=../../../../share/2016-guosai/ class=md-nav__link> 2016数学建模国赛 </a> </li> <li class=md-nav__item> <a href=../../../../share/math-model-szb/ class=md-nav__link> 数学建模之2016深圳杯——初次尝试 </a> </li> <li class=md-nav__item> <a href=../../../../share/polygon-to-ellipse/ class=md-nav__link> 随机多边形转化为椭圆的过程研究 </a> </li> <li class=md-nav__item> <a href=../../../../share/FFT-GPU-Accel/ class=md-nav__link> FFT-GPU-Accel </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5> <label class=md-nav__link for=__nav_3_5 id=__nav_3_5_label tabindex=0> NKU 数院试题整理 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_5_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> NKU 数院试题整理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-sms-exams/ class=md-nav__link> 南开数院 - 试题汇总 </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5_2> <label class=md-nav__link for=__nav_3_5_2 id=__nav_3_5_2_label tabindex=0> 分析 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5_2> <span class="md-nav__icon md-icon"></span> 分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/functional-analysis-final/ class=md-nav__link> 2017-2018第一学期泛函分析期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/real-variable-function/ class=md-nav__link> 2016-2017第二学期实变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-final/ class=md-nav__link> 2016-2017第一学期数学分析3-3期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/complex-analysis-final/ class=md-nav__link> 2016-2017第一学期复变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-middle/ class=md-nav__link> 2016-2017第一学期数学分析3-3期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-final/ class=md-nav__link> 2015-2016第二学期数学分析3-2期末考试（含解答） </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-middle/ class=md-nav__link> 2015-2016第二学期数学分析3-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-1-final/ class=md-nav__link> 2015-2016第一学期数学分析3-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5_3> <label class=md-nav__link for=__nav_3_5_3 id=__nav_3_5_3_label tabindex=0> 代数 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5_3> <span class="md-nav__icon md-icon"></span> 代数 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-final/ class=md-nav__link> 2016-2017第一学期抽象代数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-middle/ class=md-nav__link> 2016-2017第一学期抽象代数期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-final/ class=md-nav__link> 2015-2016第二学期高等代数2-2期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-middle/ class=md-nav__link> 2015-2016第二学期高等代数2-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-1-final/ class=md-nav__link> 2015-2016第一学期高等代数2-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5_4> <label class=md-nav__link for=__nav_3_5_4 id=__nav_3_5_4_label tabindex=0> 概率统计 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5_4> <span class="md-nav__icon md-icon"></span> 概率统计 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/probability-final/ class=md-nav__link> 2016-2017第二学期概率论期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/probability-middle/ class=md-nav__link> 2016-2017第二学期概率论期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/statistical-decision/ class=md-nav__link> 2020-2021第二学期统计决策期末考试(不完全回忆版) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5_5> <label class=md-nav__link for=__nav_3_5_5 id=__nav_3_5_5_label tabindex=0> 微分方程 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5_5> <span class="md-nav__icon md-icon"></span> 微分方程 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/PDE-final/ class=md-nav__link> 2017-2018第一学期数理方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-final/ class=md-nav__link> 2016-2017第一学期常微分方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-middle/ class=md-nav__link> 2016-2017第一学期常微分方程期中考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5_6> <label class=md-nav__link for=__nav_3_5_6 id=__nav_3_5_6_label tabindex=0> 机器学习导论 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5_6> <span class="md-nav__icon md-icon"></span> 机器学习导论 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/machine-learning-final/ class=md-nav__link> 2020-2021第一学期机器学习导论期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5_7> <label class=md-nav__link for=__nav_3_5_7 id=__nav_3_5_7_label tabindex=0> 优化 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_5_7_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5_7> <span class="md-nav__icon md-icon"></span> 优化 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/convex-optimization/ class=md-nav__link> 2020-2021第二学期最优化方法期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/discrete-optimization/ class=md-nav__link> 2020-2021第二学期离散优化期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5_8> <label class=md-nav__link for=__nav_3_5_8 id=__nav_3_5_8_label tabindex=0> GPU程序设计 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5_8> <span class="md-nav__icon md-icon"></span> GPU程序设计 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/GPU-programming/ class=md-nav__link> 2020-2021第二学期GPU程序设计期末考试 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_6> <label class=md-nav__link for=__nav_3_6 id=__nav_3_6_label tabindex=0> Tips <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_6_label aria-expanded=false> <label class=md-nav__title for=__nav_3_6> <span class="md-nav__icon md-icon"></span> Tips </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/tips/to-do/ class=md-nav__link> To Do </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/python/ class=md-nav__link> Python </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/data-processing/ class=md-nav__link> Data Processing </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/git/ class=md-nav__link> Git </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/linux/ class=md-nav__link> Linux </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/win/ class=md-nav__link> Windows </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/mac/ class=md-nav__link> macOS </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> Other <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Other </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../other/ class=md-nav__link> Other </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#131-policy-approximation-and-its-advantages class=md-nav__link> 13.1 Policy Approximation and its Advantages </a> </li> <li class=md-nav__item> <a href=#132-the-policy-gradient-theorem class=md-nav__link> 13.2 The Policy Gradient Theorem </a> </li> <li class=md-nav__item> <a href=#133-reinforce-monte-carlo-policy-gradient class=md-nav__link> 13.3 REINFORCE: Monte Carlo Policy Gradient </a> </li> <li class=md-nav__item> <a href=#134-reinforce-with-baseline class=md-nav__link> 13.4 REINFORCE with Baseline </a> </li> <li class=md-nav__item> <a href=#135-actorcritic-methods class=md-nav__link> 13.5 Actor–Critic Methods </a> </li> <li class=md-nav__item> <a href=#136-policy-gradient-for-continuing-problems class=md-nav__link> 13.6 Policy Gradient for Continuing Problems </a> </li> <li class=md-nav__item> <a href=#137-policy-parameterization-for-continuous-actions class=md-nav__link> 13.7 Policy Parameterization for Continuous Actions </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=->强化学习导论（十三）- 策略梯度法<a class=headerlink href=#- title="Permanent link">&para;</a></h1> <p>之前一直在讲 action-value 方法，它们都依赖于对 action-value 的估计，而本章的方法将考虑直接去学习『参数化策略』，这样就能不通过 value function 来选择 action 。</p> <div class=arithmatex>\[ \pi ( a | s , \boldsymbol { \theta } ) = \operatorname { Pr } \left\{ A _ { t } = a | S _ { t } = s , \boldsymbol { \theta } _ { t } = \boldsymbol { \theta } \right\} \]</div> <p>本章主要考虑对度量函数 <span class=arithmatex>\(J ( \boldsymbol { \theta } )\)</span> 的梯度（关于策略参数 <span class=arithmatex>\(\boldsymbol { \theta }\)</span> ）来学习，来最大化 performance ，因此参数更新式即为对 <span class=arithmatex>\(J\)</span> 的梯度上升：</p> <div class=arithmatex>\[ \boldsymbol { \theta } _ { t + 1 } = \boldsymbol { \theta } _ { t } + \alpha \widehat { \nabla J \left( \boldsymbol { \theta } _ { t } \right) } \]</div> <p>其中，<span class=arithmatex>\(\widehat { \nabla J \left( \boldsymbol { \theta } _ { t } \right) } \in \mathbb { R } ^ { d ^ { \prime } }\)</span> 是对梯度的一个估计，其期望值为该梯度。所有满足该通式的方法，均称为『policy gradient methods』。</p> <p>有些方法，既学习了 policy ，也同时学了 value function ，称这样的方法为『actor-critic methods』，其中『actor』指所学的 policy，『critic』指所学的 value function 。</p> <p>下面具体介绍 Policy Gradient Methods 。</p> <h2 id=131-policy-approximation-and-its-advantages><strong>13.1 Policy Approximation and its Advantages</strong><a class=headerlink href=#131-policy-approximation-and-its-advantages title="Permanent link">&para;</a></h2> <p>对于不太大的离散 action 空间，若要将 policy 参数化，一个很自然的方法是对每个 state-action 来构造一个参数化的数值偏好 <span class=arithmatex>\(h(s, a, \boldsymbol{\theta}) \in \mathbb{R}\)</span> ，进而通过指数 soft-max 函数来得到分布</p> <div class=arithmatex>\[ \pi(a | s, \boldsymbol{\theta}) \doteq \frac{e^{h(s, a, \boldsymbol{\theta})}}{\sum_{b} e^{h(s, b, \boldsymbol{\theta})}} \]</div> <p>称这样参数化得到的 policy 为『soft-max in action preferences』。</p> <p>action preferences 可以被任意参数化。既可以用神经网络来计算（ <span class=arithmatex>\(\boldsymbol{\theta}\)</span> 作为网络的权重），也可以简单地使用线性模型 <span class=arithmatex>\(h(s, a, \boldsymbol{\theta})=\boldsymbol{\theta}^{\top} \mathbf{x}(s, a)\)</span> 。</p> <p>Policy Approximation 有几个优点：</p> <ul> <li>第一个优势是，即使对于 deterministic policy（确定性策略，明确选择某个具体 action 的策略），参数化策略也能足够逼近（比如将某个 a 对应的 <span class=arithmatex>\(h(s,a,\boldsymbol{\theta})\)</span> 设为无穷大即可），而传统的 <span class=arithmatex>\(\varepsilon\)</span>-greedy 策略则不能做到，因为它必须对非最优策略分配 <span class=arithmatex>\(\varepsilon\)</span> 的概率。</li> <li>第二个优势是能灵活地任意分配 action 的概率，对于一些特殊情况，比如不完全信息下的卡牌游戏，最佳 policy 对应的选择随机性很强，能够对两种差异很大的 action 来分配概率进而做出选择，比如在 Poker 中进行 bluffing 时（bluff 指在自己手牌较弱时加注以试图吓退对方），这对于 action-value 方法而言很难做到，从书中 example 13.1 中可以简单明了的看出两类方法的效果差异。</li> <li>第三个优势是，参数化的 policy 是一个相对更易于近似的函数(Simsek, Algorta, and Kothiyal, 2016)。</li> <li>最后一个优势是，参数化的 policy 能较好地将先验知识引入强化学习系统，这通常也是选择 policy-based learning method 的重要原因。</li> </ul> <h2 id=132-the-policy-gradient-theorem><strong>13.2 The Policy Gradient Theorem</strong><a class=headerlink href=#132-the-policy-gradient-theorem title="Permanent link">&para;</a></h2> <p>参数化 policy 除了上一节提到的几点实用价值外，还有一个重要的理论优势。对于连续的参数化 policy ，action 的概率可以连续性地改变，而 <span class=arithmatex>\(\varepsilon\)</span>-greedy 方法中选择 action 的概率则有可能因小变动而发生突变，主要是由于 policy-gradient methods 有着更强的收敛性。</p> <p>这一节先只考虑 episodic 形式的问题，不失一般性，先假定每一段 episode 都起始于指定状态 <span class=arithmatex>\(s_0\)</span> ，定义 performance：</p> <div class=arithmatex>\[ J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right) \]</div> <p>其中 <span class=arithmatex>\(v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right)\)</span> 是 <span class=arithmatex>\(\pi_\theta\)</span> 的 <strong>true</strong> value function ，其中策略由 <span class=arithmatex>\(\boldsymbol{\theta}\)</span> 决定。为简化证明，后面的推导中假定 <span class=arithmatex>\(\gamma=1\)</span> （但描述算法时则写回一般形式）。</p> <p>『<strong>Policy Gradient Theorem</strong>』：</p> <div class=arithmatex>\[ \nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta}) \]</div> <p>推导过程如下 (episodic case)</p> <div class=arithmatex>\[ \begin{aligned} \nabla v_{\pi}(s) &amp;=\nabla\left[\sum_{a} \pi(a | s) q_{\pi}(s, a)\right], \quad \text { for all } s \in \mathcal{S} \\ &amp;=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla q_{\pi}(s, a)\right] \\ &amp;=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left(r+v_{\pi}\left(s^{\prime}\right)\right)\right] \\&amp;=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right] \\&amp;= \sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \\ &amp;\times \sum_{a^{\prime}}\left[\nabla \pi\left(a^{\prime} | s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)+\pi\left(a^{\prime} | s^{\prime}\right) \sum_{s^{\prime \prime}} p\left(s^{\prime \prime} | s^{\prime}, a^{\prime}\right) \nabla v_{\pi}\left(s^{\prime \prime}\right)\right] \Bigg]\\ &amp;=\sum_{x \in \mathcal S} \sum_{k=0}^{\infty} \operatorname{Pr}(s \rightarrow x, k, \pi) \sum_{a} \nabla \pi(a | x) q_{\pi}(x, a) \end{aligned} \]</div> <p>上一步是将前式反复展开得来，其中 <span class=arithmatex>\(\operatorname{Pr}(s \rightarrow x, k, \pi)\)</span> 表示在策略 <span class=arithmatex>\(\pi\)</span> 下，从状态 s 经过 k 步达到状态 x 的转移概率，于是可得</p> <div class=arithmatex>\[ \begin{aligned} \nabla J(\boldsymbol{\theta}) &amp;=\nabla v_{\pi}\left(s_{0}\right) \\ &amp;=\sum_{s}\left(\sum_{k=0}^{\infty} \operatorname{Pr}\left(s_{0} \rightarrow s, k, \pi\right)\right) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;=\sum_{s} \eta(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;=\sum_{s^{\prime}} \eta\left(s^{\prime}\right) \sum_{s} \frac{\eta(s)}{\sum_{s^{\prime}} \eta\left(s^{\prime}\right)} \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;=\sum_{s^{\prime}} \eta\left(s^{\prime}\right) \sum_{s} \mu(s)\sum_a\nabla\pi(a|s) q_{\pi}(s, a) \\ &amp; \propto \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \end{aligned} \]</div> <p>证毕。</p> <p>关于 <span class=arithmatex>\(\eta(s)\)</span> 和 <span class=arithmatex>\(\mu(s)\)</span> ，之前已在第 9、10 章有过相关定义：</p> <div class=arithmatex>\[ \begin{aligned} \eta(s)&amp;=h(s)+\sum_{\overline{s}} \eta(\overline{s}) \sum_{a} \pi(a | \overline{s}) p(s | \overline{s}, a), \text { for all } s \in \mathcal{S}\\ \mu(s)&amp;=\frac{\eta(s)}{\sum_{s^{\prime}} \eta\left(s^{\prime}\right)}, \text { for all } s \in \mathcal{S} \end{aligned} \]</div> <h2 id=133-reinforce-monte-carlo-policy-gradient><strong>13.3 REINFORCE: Monte Carlo Policy Gradient</strong><a class=headerlink href=#133-reinforce-monte-carlo-policy-gradient title="Permanent link">&para;</a></h2> <p>下面开始介绍 policy-gradient 的学习算法。回想一开始的目标是要得到随机梯度上升的形式，且希望样本梯度的期望恰好为度量函数的梯度，而 policy gradient theorem 给出的公式恰好满足，注意到公式右侧是一个关于 <span class=arithmatex>\(\mu(s)\)</span> （其含义是在服从策略 <span class=arithmatex>\(\pi\)</span> 时，各状态 s 发生的概率）的加权和，因此有</p> <div class=arithmatex>\[ \begin{aligned} \nabla J(\boldsymbol{\theta}) &amp; \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta}) \\ &amp;=\mathbb{E}_{\pi}\left[\sum_{a} q_{\pi}\left(S_{t}, a\right) \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)\right] \end{aligned} \]</div> <p>于是我们的随机梯度上升算法可以写作：</p> <div class=arithmatex>\[ \boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha \sum_{a} \hat{q}\left(S_{t}, a, \mathbf{w}\right) \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right) \]</div> <p>其中 <span class=arithmatex>\(\hat{q}\)</span> 是对 <span class=arithmatex>\(q_\pi\)</span> 学习出来的逼近，称该算法 all-actions 方法。因为它的更新过程包含了全部 action ，有着不错的前景，但这里将重点放在传统的 REINFORCE algorithm (Willams, 1992) ，它在 t 时刻的更新只涉及到该时刻实际采取行动的 action <span class=arithmatex>\(A_t\)</span> ，做法是将随机变量的加权和替换为一个期望，然后来对这个期望做采样：</p> <div class=arithmatex>\[ \begin{aligned} \nabla J(\boldsymbol{\theta}) &amp;=\mathbb{E}_{\pi}\left[\sum_{a} \pi\left(a | S_{t}, \boldsymbol{\theta}\right) q_{\pi}\left(S_{t}, a\right) \frac{\nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(a | S_{t}, \boldsymbol{\theta}\right)}\right] \\ &amp;=\mathbb{E}_{\pi}\left[q_{\pi}\left(S_{t}, A_{t}\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right] \\ &amp;=\mathbb{E}_{\pi}\left[G_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right] \end{aligned} \]</div> <p>上面第一步既是将随机变量 a 替换为样本 <span class=arithmatex>\(A_t\)</span> ，第二步是因为 <span class=arithmatex>\(\mathbb{E}_{\pi}\left[G_{t} | S_{t}, A_{t}\right]=q_{\pi}\left(S_{t}, A_{t}\right)\)</span> 。这样就得到了 REINFORCE update：</p> <div class=arithmatex>\[ \boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha G_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \]</div> <p>这样就能通过简单的采样来代替梯度。直观上也很好理解，每次的增量正比于 <span class=arithmatex>\(G_t\)</span> 乘上一个向量，这个向量即为参数空间中，那些能够增加 <span class=arithmatex>\(A_t\)</span> 在 <span class=arithmatex>\(S_t\)</span> 下被选中概率的参数的<strong>方向</strong>。于是如果 reward 较好，<span class=arithmatex>\(A_t\)</span> 对应的参数方向上就会得到较大幅度的更新，促进未来再被选中的概率，反之如果 reward 较差，更新的幅度就会变小，相对不如其他 action ，未来被选中的概率就会降低。</p> <p>由于 REINFORCE 算法使用了完整的返回值 <span class=arithmatex>\(G_t\)</span> ，因此属于蒙特卡罗算法，只适用于 episodic 形式。</p> <p><img alt src=../imgs/RLAI_13/REINFORCE_algorighm.png></p> <p>注意最后一行用 <span class=arithmatex>\(\nabla \ln \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)\)</span> 来代替 <span class=arithmatex>\(\frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}\)</span> ，这是利用了 <span class=arithmatex>\(\nabla \ln x=\frac{\nabla x}{x}\)</span> 的简单性质。</p> <h2 id=134-reinforce-with-baseline><strong>13.4 REINFORCE with Baseline</strong><a class=headerlink href=#134-reinforce-with-baseline title="Permanent link">&para;</a></h2> <p>Policy gradient theorem 原本的形式为：</p> <div class=arithmatex>\[ \nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta}) \]</div> <p>考虑加入一个任意的 baseline <span class=arithmatex>\(b(s)\)</span> ：</p> <div class=arithmatex>\[ \nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a}\left(q_{\pi}(s, a)-b(s)\right) \nabla \pi(a | s, \boldsymbol{\theta}) \]</div> <p><span class=arithmatex>\(b(s)\)</span> 可以是任意函数或随机变量，只要不和 a 相关即可。这样的改动显然是合理的，因为</p> <div class=arithmatex>\[ \sum_{a} b(s) \nabla \pi(a | s, \boldsymbol{\theta})=b(s) \nabla \sum_{a} \pi(a | s, \boldsymbol{\theta})=b(s) \nabla 1=0 \]</div> <p>这样便能同理得到带 baseline 的 REINFORCE 更新式：</p> <div class=arithmatex>\[ \boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha\left(G_{t}-b\left(S_{t}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \]</div> <p>加入 baseline 不会对期望有任何影响，但能够减小方差。常用的选择是将 <span class=arithmatex>\(\hat{v}\left(S_{t}, \mathbf{w}\right)\)</span> 作为 baseline ，好处是无需做别的计算，利用现有的量就能同时更新 <span class=arithmatex>\(\boldsymbol \theta\)</span> 和 <span class=arithmatex>\(\boldsymbol w\)</span> 。具体算法如下：</p> <p><img alt src=../imgs/RLAI_13/REINFORCE_algorighm_baseline.png></p> <h2 id=135-actorcritic-methods><strong>13.5 Actor–Critic Methods</strong><a class=headerlink href=#135-actorcritic-methods title="Permanent link">&para;</a></h2> <p>最开头提到过，同时学习 policy 和 value function 的算法是『actor-critic 算法』，尽管上一节的 REINFORCE-with-baseline 算法同时学习了 policy 和 value function ，但并不认为它是 actor-critic 算法，因为这个 value function 仅仅是用作 baseline ，而没起到 critic 的用处，具体而言，即是它没有用来做 bootstrapping（指通过状态估计值序列来更新状态估计值。例如用 <span class=arithmatex>\(R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)\)</span> 更新就是 bootstrapping，而用 <span class=arithmatex>\(G_t -\hat{v}\left(S_{t}, \mathbf{w}\right)\)</span> 更新则不是）。</p> <p>这样区分是有意义的，因为只有通过 bootstrapping 才能引入 bias ，以及对函数近似效果的渐进依赖。前面章节有提到过，通过 bootstrapping 引入的 bias 以及对状态表示的依赖都是有益的，因为它能够减小方差，并且加速学习。而上一节的 REINFORCE-with-baseline 算法由于是 unbiased 的，容易渐进收敛到局部最优解，同时由于是 MC 方法，方差较大，学习速度较慢，且不太适合处理在线学习/连续型问题。之前章节介绍的 TD 方法恰好能够解决上面所有的缺点，所以引出了下面的 actor–critic methods with a bootstrapping critic 。</p> <p>首先考虑 one-step actor-critic methods ，将 full return 替换为 one-step return:</p> <div class=arithmatex>\[ \begin{aligned} \boldsymbol{\theta}_{t+1} &amp; \doteq \boldsymbol{\theta}_{t}+\alpha\left(G_{t : t+1}-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \\ &amp;=\boldsymbol{\theta}_{t}+\alpha\left(R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \\ &amp;=\boldsymbol{\theta}_{t}+\alpha \delta_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \end{aligned} \]</div> <p>下面是算法伪码：</p> <p><img alt src=../imgs/RLAI_13/one-step-actor-critic.png></p> <h2 id=136-policy-gradient-for-continuing-problems><strong>13.6 Policy Gradient for Continuing Problems</strong><a class=headerlink href=#136-policy-gradient-for-continuing-problems title="Permanent link">&para;</a></h2> <p>在第十章中，对于没有 episode 界限的连续型问题，定义了平均回报率：</p> <div class=arithmatex>\[ \begin{aligned} J(\boldsymbol{\theta}) \doteq r(\pi) &amp; \doteq \lim _{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &amp;=\lim _{t \rightarrow \infty} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &amp;=\sum_{s} \mu(s) \sum_{a} \pi(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right) r \end{aligned} \]</div> <p>其中 <span class=arithmatex>\(\mu(s) \doteq \lim _{t \rightarrow \infty} \operatorname{Pr}\left\{S_{t}=s | A_{0 : t} \sim \pi\right\}\)</span> 。</p> <p>此时</p> <div class=arithmatex>\[ G_{t} \doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\cdots \]</div> <p>使用上面在连续型问题下定义的 return 值，原先 episodic 下的 Policy Gradient Theorem 便可拓展到连续型问题下了。</p> <p>下面给出连续型问题下 Policy Gradient Theorem 的证明</p> <div class=arithmatex>\[ \begin{aligned} \nabla v_{\pi}(s) &amp;=\nabla\left[\sum_{a} \pi(a | s) q_{\pi}(s, a)\right], \quad \text { for all } s \in \mathcal{S} \\ &amp;=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla q_{\pi}(s, a)\right]\\ &amp;=\sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left(r-r(\boldsymbol{\theta})+v_{\pi}\left(s^{\prime}\right)\right)\Bigg] \\ &amp;=\sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s)\left[-\nabla r(\boldsymbol{\theta})+\sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]\Bigg] \end{aligned} \]</div> <p>整理可得</p> <div class=arithmatex>\[ \nabla r(\boldsymbol{\theta})=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]-\nabla v_{\pi}(s) \]</div> <p>注意到等式坐标即为 <span class=arithmatex>\(\nabla J(\boldsymbol{\theta})\)</span> ，与 s 无关，故等式右边也与 s 无关，且由于 <span class=arithmatex>\(\sum_{s} \mu(s)=1\)</span> ，可得</p> <div class=arithmatex>\[ \begin{aligned} \nabla J(\boldsymbol{\theta})=&amp; \sum_{s} \mu(s)\Bigg(\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]-\nabla v_{\pi}(s)\Bigg) \\=&amp; \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;+\sum_{s} \mu(s) \sum_{a} \pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s) \\ =&amp; \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;+\sum_{s^{\prime}} \underbrace{\sum_{s} \mu(s) \sum_{a} \pi(a | s) p\left(s^{\prime} | s, a\right)}_{\mu(s^{\prime})} \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s) \end{aligned} \]</div> <p>整理即得</p> <div class=arithmatex>\[ \begin{aligned} \nabla J(\boldsymbol{\theta})=&amp; \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a)+\sum_{s^{\prime}} \mu\left(s^{\prime}\right) \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s) \\=&amp; \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \end{aligned} \]</div> <h2 id=137-policy-parameterization-for-continuous-actions><strong>13.7 Policy Parameterization for Continuous Actions</strong><a class=headerlink href=#137-policy-parameterization-for-continuous-actions title="Permanent link">&para;</a></h2> <p>Policy-based methods 为较大 action 空间的问题提供了实用的处理方法，甚至对于连续型问题这种有着无穷种 action 的情况也没问题，它并不去计算某个具体 action 的概率值，而是直接去学习概率分布。例如，假设 action 集合是一些实数，并且来自一个高斯分布，其概率分布便可写作</p> <div class=arithmatex>\[ \pi(a | s, \boldsymbol{\theta}) \doteq \frac{1}{\sigma(s, \boldsymbol{\theta}) \sqrt{2 \pi}} \exp \left(-\frac{(a-\mu(s, \boldsymbol{\theta}))^{2}}{2 \sigma(s, \boldsymbol{\theta})^{2}}\right) \]</div> <p>其中 <span class=arithmatex>\(\mu : \mathcal{S} \times \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}, \sigma : \mathcal{S} \times \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}^{+}\)</span> 是参数化的近似函数。策略参数 <span class=arithmatex>\(\boldsymbol{\theta}\)</span> 由两部分组成：<span class=arithmatex>\(\boldsymbol{\theta}=\left[\boldsymbol{\theta}_{\mu}, \boldsymbol{\theta}_{\sigma}\right]^{\top}\)</span> ，第一部分用于均值的近似，第二部分用于标准差的近似：</p> <div class=arithmatex>\[ \mu(s, \boldsymbol{\theta}) \doteq \boldsymbol{\theta}_{\mu}^{\top} \mathbf{x}_{\mu}(s) \quad \text { and } \quad \sigma(s, \boldsymbol{\theta}) \doteq \exp \left(\boldsymbol{\theta}_{\sigma}^{\top} \mathbf{x}_{\sigma}(s)\right) \]</div> <p>这样，便组成了完整的连续型问题下 Policy 参数化算法。</p> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">July 26, 2020</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016-2023 ZHANGWP </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/zawnpn target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/zawnpn target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://www.linkedin.com/in/zawnpn/ target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href=https://www.zhihu.com/people/zhangwanpeng target=_blank rel=noopener title=www.zhihu.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13H170.54zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82v170.31zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62v.01zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2l19.23 14.43zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78l.03-.01z"/></svg> </a> <a href=https://psnprofiles.com/zawnpn target=_blank rel=noopener title=psnprofiles.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M570.9 372.3c-11.3 14.2-38.8 24.3-38.8 24.3L327 470.2v-54.3l150.9-53.8c17.1-6.1 19.8-14.8 5.8-19.4-13.9-4.6-39.1-3.3-56.2 2.9L327 381.1v-56.4c23.2-7.8 47.1-13.6 75.7-16.8 40.9-4.5 90.9.6 130.2 15.5 44.2 14 49.2 34.7 38 48.9zm-224.4-92.5v-139c0-16.3-3-31.3-18.3-35.6-11.7-3.8-19 7.1-19 23.4v347.9l-93.8-29.8V32c39.9 7.4 98 24.9 129.2 35.4C424.1 94.7 451 128.7 451 205.2c0 74.5-46 102.8-104.5 74.6zM43.2 410.2c-45.4-12.8-53-39.5-32.3-54.8 19.1-14.2 51.7-24.9 51.7-24.9l134.5-47.8v54.5l-96.8 34.6c-17.1 6.1-19.7 14.8-5.8 19.4 13.9 4.6 39.1 3.3 56.2-2.9l46.4-16.9v48.8c-51.6 9.3-101.4 7.3-153.9-10z"/></svg> </a> <a href=https://steamcommunity.com/id/zawnpn/ target=_blank rel=noopener title=steamcommunity.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M496 256c0 137-111.2 248-248.4 248-113.8 0-209.6-76.3-239-180.4l95.2 39.3c6.4 32.1 34.9 56.4 68.9 56.4 39.2 0 71.9-32.4 70.2-73.5l84.5-60.2c52.1 1.3 95.8-40.9 95.8-93.5 0-51.6-42-93.5-93.7-93.5s-93.7 42-93.7 93.5v1.2L176.6 279c-15.5-.9-30.7 3.4-43.5 12.1L0 236.1C10.2 108.4 117.1 8 247.6 8 384.8 8 496 119 496 256zM155.7 384.3l-30.5-12.6a52.79 52.79 0 0 0 27.2 25.8c26.9 11.2 57.8-1.6 69-28.4 5.4-13 5.5-27.3.1-40.3-5.4-13-15.5-23.2-28.5-28.6-12.9-5.4-26.7-5.2-38.9-.6l31.5 13c19.8 8.2 29.2 30.9 20.9 50.7-8.3 19.9-31 29.2-50.8 21zm173.8-129.9c-34.4 0-62.4-28-62.4-62.3s28-62.3 62.4-62.3 62.4 28 62.4 62.3-27.9 62.3-62.4 62.3zm.1-15.6c25.9 0 46.9-21 46.9-46.8 0-25.9-21-46.8-46.9-46.8s-46.9 21-46.9 46.8c.1 25.8 21.1 46.8 46.9 46.8z"/></svg> </a> <a href=https://space.bilibili.com/38332230 target=_blank rel=noopener title=space.bilibili.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M0 256a256 256 0 1 1 512 0 256 256 0 1 1-512 0zm188.3-108.9c-7.6 4.2-12.3 12.3-12.3 20.9v176c0 8.7 4.7 16.7 12.3 20.9s16.8 4.1 24.3-.5l144-88c7.1-4.4 11.5-12.1 11.5-20.5s-4.4-16.1-11.5-20.5l-144-88c-7.4-4.5-16.7-4.7-24.3-.5z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../..", "features": ["navigation.tabs", "navigation.prune"], "search": "../../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../assets/javascripts/bundle.51198bba.min.js></script> <script src=../../../../_static/extra.js></script> <script src=//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js></script> <script src=//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script> </body> </html>
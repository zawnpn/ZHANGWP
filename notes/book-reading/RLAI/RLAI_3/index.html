<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to zhangwp's blog."><meta name=author content="Wanpeng Zhang"><link href=https://www.zhangwp.com/notes/book-reading/RLAI/RLAI_3/ rel=canonical><link rel=icon href=../../../../_static/images/favicon.svg><meta name=generator content="mkdocs-1.3.0, mkdocs-material-8.3.9"><title>Chapter 3 - ZHANGWP</title><link rel=stylesheet href=../../../../assets/stylesheets/main.1d29e8d0.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.cbb835fc.min.css><meta name=theme-color content=#ffffff><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Noto Sans SC";--md-code-font:"Fira Code"}</style><script>__md_scope=new URL("../../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=light-blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#- class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title=ZHANGWP class="md-header__button md-logo" aria-label=ZHANGWP data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M9.372 86.63c-12.496-12.5-12.496-32.76 0-45.26 12.498-12.49 32.758-12.49 45.258 0L246.6 233.4c12.5 12.5 12.5 32.7 0 45.2l-191.97 192c-12.5 12.5-32.76 12.5-45.258 0-12.496-12.5-12.496-32.7 0-45.2L178.7 256 9.372 86.63zM544 416c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32h288z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> ZHANGWP </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Chapter 3 </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/zawnpn/ZHANGWP title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M160 80c0 32.8-19.7 60.1-48 73.3v87.8c18.8-10.9 40.7-17.1 64-17.1h96c35.3 0 64-28.7 64-64v-6.7c-28.3-13.2-48-40.5-48-73.3 0-44.18 35.8-80 80-80s80 35.82 80 80c0 32.8-19.7 60.1-48 73.3v6.7c0 70.7-57.3 128-128 128h-96c-35.3 0-64 28.7-64 64v6.7c28.3 12.3 48 40.5 48 73.3 0 44.2-35.8 80-80 80-44.18 0-80-35.8-80-80 0-32.8 19.75-61 48-73.3V153.3C19.75 140.1 0 112.8 0 80 0 35.82 35.82 0 80 0c44.2 0 80 35.82 80 80zm-80 24c13.25 0 24-10.75 24-24S93.25 56 80 56 56 66.75 56 80s10.75 24 24 24zm288-48c-13.3 0-24 10.75-24 24s10.7 24 24 24 24-10.75 24-24-10.7-24-24-24zM80 456c13.25 0 24-10.7 24-24s-10.75-24-24-24-24 10.7-24 24 10.75 24 24 24z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../ class="md-tabs__link md-tabs__link--active"> Note </a> </li> <li class=md-tabs__item> <a href=../../../../share/ class=md-tabs__link> Share </a> </li> <li class=md-tabs__item> <a href=../../../../other/ class=md-tabs__link> Other </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title=ZHANGWP class="md-nav__button md-logo" aria-label=ZHANGWP data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M9.372 86.63c-12.496-12.5-12.496-32.76 0-45.26 12.498-12.49 32.758-12.49 45.258 0L246.6 233.4c12.5 12.5 12.5 32.7 0 45.2l-191.97 192c-12.5 12.5-32.76 12.5-45.258 0-12.496-12.5-12.496-32.7 0-45.2L178.7 256 9.372 86.63zM544 416c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32h288z"/></svg> </a> ZHANGWP </label> <div class=md-nav__source> <a href=https://github.com/zawnpn/ZHANGWP title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M160 80c0 32.8-19.7 60.1-48 73.3v87.8c18.8-10.9 40.7-17.1 64-17.1h96c35.3 0 64-28.7 64-64v-6.7c-28.3-13.2-48-40.5-48-73.3 0-44.18 35.8-80 80-80s80 35.82 80 80c0 32.8-19.7 60.1-48 73.3v6.7c0 70.7-57.3 128-128 128h-96c-35.3 0-64 28.7-64 64v6.7c28.3 12.3 48 40.5 48 73.3 0 44.2-35.8 80-80 80-44.18 0-80-35.8-80-80 0-32.8 19.75-61 48-73.3V153.3C19.75 140.1 0 112.8 0 80 0 35.82 35.82 0 80 0c44.2 0 80 35.82 80 80zm-80 24c13.25 0 24-10.75 24-24S93.25 56 80 56 56 66.75 56 80s10.75 24 24 24zm288-48c-13.3 0-24 10.75-24 24s10.7 24 24 24 24-10.75 24-24-10.7-24-24-24zM80 456c13.25 0 24-10.7 24-24s-10.75-24-24-24-24 10.7-24 24 10.75 24 24 24z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_1 type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1> Home <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. class=md-nav__link> Home </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2 type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2> Note <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Note data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Note </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_2 type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2> Paper Reading <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Paper Reading" data-md-level=2> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Paper Reading </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_2_1 type=checkbox id=__nav_2_2_1> <label class=md-nav__link for=__nav_2_2_1> Model-based RL <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Model-based RL" data-md-level=3> <label class=md-nav__title for=__nav_2_2_1> <span class="md-nav__icon md-icon"></span> Model-based RL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../paper-reading/MBRL_with_uncertainty/ class=md-nav__link> MBRL with uncertainty </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_2_2 type=checkbox id=__nav_2_2_2> <label class=md-nav__link for=__nav_2_2_2> RL Control <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="RL Control" data-md-level=3> <label class=md-nav__title for=__nav_2_2_2> <span class="md-nav__icon md-icon"></span> RL Control </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../paper-reading/MCTS_introduction/ class=md-nav__link> MCTS introduction </a> </li> <li class=md-nav__item> <a href=../../../paper-reading/background_and_decision-time_planning/ class=md-nav__link> Background and decision-time planning </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_3 type=checkbox id=__nav_2_3 checked> <label class=md-nav__link for=__nav_2_3> Book Reading <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Book Reading" data-md-level=2> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Book Reading </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_3_1 type=checkbox id=__nav_2_3_1 checked> <label class=md-nav__link for=__nav_2_3_1> Reinforcement Learning An Introduction <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Reinforcement Learning An Introduction" data-md-level=3> <label class=md-nav__title for=__nav_2_3_1> <span class="md-nav__icon md-icon"></span> Reinforcement Learning An Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../RLAI_2/ class=md-nav__link> Chapter 2 </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Chapter 3 <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Chapter 3 </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#31-the-agentenvironment-interface class=md-nav__link> 3.1 The Agent–Environment Interface </a> <nav class=md-nav aria-label="3.1 The Agent–Environment Interface"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-recycling-robot class=md-nav__link> Example: Recycling Robot </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#32-goals-and-rewards class=md-nav__link> 3.2 Goals and Rewards </a> </li> <li class=md-nav__item> <a href=#33-returns class=md-nav__link> 3.3 Returns </a> <nav class=md-nav aria-label="3.3 Returns"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#episodic-tasks class=md-nav__link> Episodic Tasks </a> </li> <li class=md-nav__item> <a href=#continuing-tasks class=md-nav__link> Continuing Tasks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#34-unified-notation-for-episodic-and-continuing-tasks class=md-nav__link> 3.4 Unified Notation for Episodic and Continuing Tasks </a> </li> <li class=md-nav__item> <a href=#35-the-markov-property class=md-nav__link> 3.5 The Markov Property </a> <nav class=md-nav aria-label="3.5 The Markov Property"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#markov-property class=md-nav__link> Markov Property </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#36-markov-decision-processes class=md-nav__link> 3.6 Markov Decision Processes </a> <nav class=md-nav aria-label="3.6 Markov Decision Processes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-recycling-robot-mdp class=md-nav__link> Example: Recycling Robot MDP </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#37-value-functions class=md-nav__link> 3.7 Value Functions </a> <nav class=md-nav aria-label="3.7 Value Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#monte-carlo-methods class=md-nav__link> Monte Carlo Methods </a> </li> <li class=md-nav__item> <a href=#bellman-equation class=md-nav__link> Bellman Equation </a> </li> <li class=md-nav__item> <a href=#example-gridworld class=md-nav__link> Example: Gridworld </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#38-optimal-value-functions class=md-nav__link> 3.8 Optimal Value Functions </a> <nav class=md-nav aria-label="3.8 Optimal Value Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bellman-optimality-equation class=md-nav__link> Bellman Optimality Equation </a> </li> <li class=md-nav__item> <a href=#example-solving-the-gridworld class=md-nav__link> Example: Solving the Gridworld </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#39-optimality-and-approximation class=md-nav__link> 3.9 Optimality and Approximation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../RLAI_4/ class=md-nav__link> Chapter 4 </a> </li> <li class=md-nav__item> <a href=../RLAI_5/ class=md-nav__link> Chapter 5 </a> </li> <li class=md-nav__item> <a href=../RLAI_6/ class=md-nav__link> Chapter 6 </a> </li> <li class=md-nav__item> <a href=../RLAI_7/ class=md-nav__link> Chapter 7 </a> </li> <li class=md-nav__item> <a href=../RLAI_8/ class=md-nav__link> Chapter 8 </a> </li> <li class=md-nav__item> <a href=../RLAI_9/ class=md-nav__link> Chapter 9 </a> </li> <li class=md-nav__item> <a href=../RLAI_10/ class=md-nav__link> Chapter 10 </a> </li> <li class=md-nav__item> <a href=../RLAI_11/ class=md-nav__link> Chapter 11 </a> </li> <li class=md-nav__item> <a href=../RLAI_12/ class=md-nav__link> Chapter 12 </a> </li> <li class=md-nav__item> <a href=../RLAI_13/ class=md-nav__link> Chapter 13 </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3 type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3> Share <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Share data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Share </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_2 type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2> Projects <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Projects data-md-level=2> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Projects </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_2_1 type=checkbox id=__nav_3_2_1> <label class=md-nav__link for=__nav_3_2_1> AI <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=AI data-md-level=3> <label class=md-nav__title for=__nav_3_2_1> <span class="md-nav__icon md-icon"></span> AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/projects/rl_runfast/ class=md-nav__link> RL Runfast </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_2_2 type=checkbox id=__nav_3_2_2> <label class=md-nav__link for=__nav_3_2_2> Typesetting <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Typesetting data-md-level=3> <label class=md-nav__title for=__nav_3_2_2> <span class="md-nav__icon md-icon"></span> Typesetting </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/projects/markdown-toolkit/ class=md-nav__link> Markdown 编译转换工具 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_2_3 type=checkbox id=__nav_3_2_3> <label class=md-nav__link for=__nav_3_2_3> NKU <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=NKU data-md-level=3> <label class=md-nav__title for=__nav_3_2_3> <span class="md-nav__icon md-icon"></span> NKU </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/projects/nku-eamis/ class=md-nav__link> NKU-EAMIS工具 </a> </li> <li class=md-nav__item> <a href=../../../../share/projects/nku-sms-rss/ class=md-nav__link> NKU-SMS-RSS </a> </li> <li class=md-nav__item> <a href=../../../../share/projects/eamis-miniapp/ class=md-nav__link> NKU-EAMIS_MiniApp(南开大学教务助手小程序) </a> </li> <li class=md-nav__item> <a href=../../../../share/projects/eamis-workflow/ class=md-nav__link> NKU-EAMIS for iOS(Workflow) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_2_4 type=checkbox id=__nav_3_2_4> <label class=md-nav__link for=__nav_3_2_4> Game <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Game data-md-level=3> <label class=md-nav__title for=__nav_3_2_4> <span class="md-nav__icon md-icon"></span> Game </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/projects/steam-market-price-bot/ class=md-nav__link> Steam市场比价爬虫 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_3 type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3> Diary <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Diary data-md-level=2> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Diary </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/roc-fly/ class=md-nav__link> 鹏程万里 </a> </li> <li class=md-nav__item> <a href=../../../../share/blog-history/ class=md-nav__link> 博客历史 </a> </li> <li class=md-nav__item> <a href=../../../../share/game-log/ class=md-nav__link> Game-Log </a> </li> <li class=md-nav__item> <a href=../../../../share/my-postgraduate-share/ class=md-nav__link> 保研推免经验分享 - 数学系跨保 CS </a> </li> <li class=md-nav__item> <a href=../../../../share/github-student-pack/ class=md-nav__link> Student Developer Pack - GitHub Education </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_4 type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4> 数学建模 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=数学建模 data-md-level=2> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> 数学建模 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/2017-mcm-icm/ class=md-nav__link> 2017美赛参赛整理(Problem D) </a> </li> <li class=md-nav__item> <a href=../../../../share/2016-guosai/ class=md-nav__link> 2016数学建模国赛 </a> </li> <li class=md-nav__item> <a href=../../../../share/math-model-szb/ class=md-nav__link> 数学建模之2016深圳杯——初次尝试 </a> </li> <li class=md-nav__item> <a href=../../../../share/polygon-to-ellipse/ class=md-nav__link> 随机多边形转化为椭圆的过程研究 </a> </li> <li class=md-nav__item> <a href=../../../../share/FFT-GPU-Accel/ class=md-nav__link> FFT-GPU-Accel </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5 type=checkbox id=__nav_3_5> <label class=md-nav__link for=__nav_3_5> NKU 数院试题整理 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="NKU 数院试题整理" data-md-level=2> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> NKU 数院试题整理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-sms-exams/ class=md-nav__link> 南开数院 - 试题汇总 </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5_2 type=checkbox id=__nav_3_5_2> <label class=md-nav__link for=__nav_3_5_2> 分析 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=分析 data-md-level=3> <label class=md-nav__title for=__nav_3_5_2> <span class="md-nav__icon md-icon"></span> 分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/functional-analysis-final/ class=md-nav__link> 2017-2018第一学期泛函分析期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/real-variable-function/ class=md-nav__link> 2016-2017第二学期实变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-final/ class=md-nav__link> 2016-2017第一学期数学分析3-3期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/complex-analysis-final/ class=md-nav__link> 2016-2017第一学期复变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-middle/ class=md-nav__link> 2016-2017第一学期数学分析3-3期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-final/ class=md-nav__link> 2015-2016第二学期数学分析3-2期末考试（含解答） </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-middle/ class=md-nav__link> 2015-2016第二学期数学分析3-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-1-final/ class=md-nav__link> 2015-2016第一学期数学分析3-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5_3 type=checkbox id=__nav_3_5_3> <label class=md-nav__link for=__nav_3_5_3> 代数 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=代数 data-md-level=3> <label class=md-nav__title for=__nav_3_5_3> <span class="md-nav__icon md-icon"></span> 代数 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-final/ class=md-nav__link> 2016-2017第一学期抽象代数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-middle/ class=md-nav__link> 2016-2017第一学期抽象代数期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-final/ class=md-nav__link> 2015-2016第二学期高等代数2-2期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-middle/ class=md-nav__link> 2015-2016第二学期高等代数2-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-1-final/ class=md-nav__link> 2015-2016第一学期高等代数2-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5_4 type=checkbox id=__nav_3_5_4> <label class=md-nav__link for=__nav_3_5_4> 概率统计 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=概率统计 data-md-level=3> <label class=md-nav__title for=__nav_3_5_4> <span class="md-nav__icon md-icon"></span> 概率统计 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/probability-final/ class=md-nav__link> 2016-2017第二学期概率论期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/probability-middle/ class=md-nav__link> 2016-2017第二学期概率论期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/statistical-decision/ class=md-nav__link> 2020-2021第二学期统计决策期末考试(不完全回忆版) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5_5 type=checkbox id=__nav_3_5_5> <label class=md-nav__link for=__nav_3_5_5> 微分方程 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=微分方程 data-md-level=3> <label class=md-nav__title for=__nav_3_5_5> <span class="md-nav__icon md-icon"></span> 微分方程 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/PDE-final/ class=md-nav__link> 2017-2018第一学期数理方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-final/ class=md-nav__link> 2016-2017第一学期常微分方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-middle/ class=md-nav__link> 2016-2017第一学期常微分方程期中考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5_6 type=checkbox id=__nav_3_5_6> <label class=md-nav__link for=__nav_3_5_6> 机器学习导论 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=机器学习导论 data-md-level=3> <label class=md-nav__title for=__nav_3_5_6> <span class="md-nav__icon md-icon"></span> 机器学习导论 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/machine-learning-final/ class=md-nav__link> 2020-2021第一学期机器学习导论期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5_7 type=checkbox id=__nav_3_5_7> <label class=md-nav__link for=__nav_3_5_7> 优化 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=优化 data-md-level=3> <label class=md-nav__title for=__nav_3_5_7> <span class="md-nav__icon md-icon"></span> 优化 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/convex-optimization/ class=md-nav__link> 2020-2021第二学期最优化方法期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/discrete-optimization/ class=md-nav__link> 2020-2021第二学期离散优化期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5_8 type=checkbox id=__nav_3_5_8> <label class=md-nav__link for=__nav_3_5_8> GPU程序设计 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=GPU程序设计 data-md-level=3> <label class=md-nav__title for=__nav_3_5_8> <span class="md-nav__icon md-icon"></span> GPU程序设计 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/GPU-programming/ class=md-nav__link> 2020-2021第二学期GPU程序设计期末考试 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6 type=checkbox id=__nav_3_6> <label class=md-nav__link for=__nav_3_6> Tips <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Tips data-md-level=2> <label class=md-nav__title for=__nav_3_6> <span class="md-nav__icon md-icon"></span> Tips </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/tips/to-do/ class=md-nav__link> To Do </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/python/ class=md-nav__link> Python </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/data-processing/ class=md-nav__link> Data Processing </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/git/ class=md-nav__link> Git </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/linux/ class=md-nav__link> Linux </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/win/ class=md-nav__link> Windows </a> </li> <li class=md-nav__item> <a href=../../../../share/tips/mac/ class=md-nav__link> macOS </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_4 type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4> Other <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Other data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Other </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../other/ class=md-nav__link> Other </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#31-the-agentenvironment-interface class=md-nav__link> 3.1 The Agent–Environment Interface </a> <nav class=md-nav aria-label="3.1 The Agent–Environment Interface"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-recycling-robot class=md-nav__link> Example: Recycling Robot </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#32-goals-and-rewards class=md-nav__link> 3.2 Goals and Rewards </a> </li> <li class=md-nav__item> <a href=#33-returns class=md-nav__link> 3.3 Returns </a> <nav class=md-nav aria-label="3.3 Returns"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#episodic-tasks class=md-nav__link> Episodic Tasks </a> </li> <li class=md-nav__item> <a href=#continuing-tasks class=md-nav__link> Continuing Tasks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#34-unified-notation-for-episodic-and-continuing-tasks class=md-nav__link> 3.4 Unified Notation for Episodic and Continuing Tasks </a> </li> <li class=md-nav__item> <a href=#35-the-markov-property class=md-nav__link> 3.5 The Markov Property </a> <nav class=md-nav aria-label="3.5 The Markov Property"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#markov-property class=md-nav__link> Markov Property </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#36-markov-decision-processes class=md-nav__link> 3.6 Markov Decision Processes </a> <nav class=md-nav aria-label="3.6 Markov Decision Processes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-recycling-robot-mdp class=md-nav__link> Example: Recycling Robot MDP </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#37-value-functions class=md-nav__link> 3.7 Value Functions </a> <nav class=md-nav aria-label="3.7 Value Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#monte-carlo-methods class=md-nav__link> Monte Carlo Methods </a> </li> <li class=md-nav__item> <a href=#bellman-equation class=md-nav__link> Bellman Equation </a> </li> <li class=md-nav__item> <a href=#example-gridworld class=md-nav__link> Example: Gridworld </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#38-optimal-value-functions class=md-nav__link> 3.8 Optimal Value Functions </a> <nav class=md-nav aria-label="3.8 Optimal Value Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bellman-optimality-equation class=md-nav__link> Bellman Optimality Equation </a> </li> <li class=md-nav__item> <a href=#example-solving-the-gridworld class=md-nav__link> Example: Solving the Gridworld </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#39-optimality-and-approximation class=md-nav__link> 3.9 Optimality and Approximation </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=->强化学习导论（三）- 有限马尔可夫决策过程<a class=headerlink href=#- title="Permanent link">&para;</a></h1> <p>这章主要讲有限马尔可夫决策过程（finite MDPs）。</p> <p>首先会涉及到上一章提到的评估反馈，但与之前不同的是，现在也会开始考虑问题与环境的联系，也就是在不同情景下去做不同的选择。MDPs 是决策序列的一种经典形式化模型，其中的行动不仅会影响当前的即时奖励值，也会通过未来的奖励值来影响后续的情况或状态。</p> <h2 id=31-the-agentenvironment-interface>3.1 The Agent–Environment Interface<a class=headerlink href=#31-the-agentenvironment-interface title="Permanent link">&para;</a></h2> <ul> <li><strong>agent</strong>: 学习者/决策者</li> <li><strong>environment</strong>: agent 与外界进行交互的物体的组合</li> </ul> <p>Agent 和 environment 在一系列的时间点上进行交互：当时间为 <span class=arithmatex>\(t(t = 0, 1, 2, 3, \ldots)\)</span>，agent 接收到环境的状态表示：<span class=arithmatex>\(S_t \in \mathcal{S}\)</span> ，在此基础上选择了一个行动 <span class=arithmatex>\(A_t \in \mathcal{A}(s)\)</span> ，之后 agent 会得到作为行动反馈的数值奖励 <span class=arithmatex>\(R_{t+1}\in\mathcal{R}\subset\mathbb{R}\)</span>，并进入新状态 <span class=arithmatex>\(S_{t+1}\)</span>。下图描绘了这样一个往复交替的过程。</p> <p><img alt=agent-env src=../imgs/RLAI_3/agent-env.png></p> <p>关于如何划清 agent 和 environment 的界限，有一个通用的准则：<strong>不能被 agent 任意改变的事物，均认为其属于 environment</strong>。Agent 往往能够得知很多外界的信息，甚至能知道奖励值关于其行动和状态的具体函数，但我们还是得把奖励值的计算过程放在 agent 外界，而不能让他自己进行计算，这是因为，正是这个奖励机制定义了 agent 所要学习去处理的任务，所以这个任务必须是超出 agent 控制能力的，反之则本末倒置了，如果 agent 能自我计算/改变 reward ，那他会按照自己的一些「理解」去学习问题，而没有面对问题的「客观本质」去学习。</p> <p>所以，agent 与 environment 的界限，代表着 <strong>学习者/决策体 的绝对控制权，而不是其掌握的 知识/信息</strong>。而一旦确定好三个关键因素「<strong>状态</strong>、<strong>行动</strong> 和 <strong>奖励</strong>」，就意味着这个界限已被确定下来。</p> <p>任何<strong>对受目标引导的行为</strong>的学习问题，都可以简化为三个<strong>信号</strong>在 agent 和 environment 间前后传递的模型：</p> <ul> <li><strong>actions</strong>: agent 的决策行动</li> <li><strong>states</strong>: 进行行动选择的基准</li> <li><strong>rewards</strong>: agent 的目标</li> </ul> <h3 id=example-recycling-robot>Example: Recycling Robot<a class=headerlink href=#example-recycling-robot title="Permanent link">&para;</a></h3> <p>前面的讲述可能有些抽象，这里举个实际的例子（清洁机器人）来看看 actions、states、rewards 在具体问题中是什么。这个机器人的主要目标是要持续清洁垃圾，但还需注意电量不被用完，所以在一些情况下得注意要回去充电，这就是问题背景，后面还会反复提到这个例子。</p> <p><strong>Actions</strong>:</p> <ul> <li>活动起来去搜寻垃圾（耗电）</li> <li>保持在原地等人来捡垃圾（不耗电）</li> <li>回去充电</li> </ul> <p><strong>States</strong>:</p> <ul> <li>电池状态</li> </ul> <p><strong>Rewards</strong>:</p> <ul> <li>如果清洁了垃圾，反馈一个正值作为奖励</li> <li>如果耗尽电量而没能及时返回充电，反馈一个较大负值作为惩罚</li> <li>其余情况反馈 0 奖励值</li> </ul> <p>这只是简单举个例子，用来给大家看看实际情况下是如何去通过这三个信号来定义一个问题。</p> <h2 id=32-goals-and-rewards>3.2 Goals and Rewards<a class=headerlink href=#32-goals-and-rewards title="Permanent link">&para;</a></h2> <p>在强化学习中，agent 的目标通过 environment 传入的奖励信号来量化，具体而言，目标即为<strong>最大化奖励值的累积求和</strong>。换一种说法，不同的奖励值决定了不同的目标、不同的学习问题。</p> <p>这里举个简单的例子作为对比：</p> <ul> <li>让机器人学会走路：提供与行走距离成正比的奖励值作为机器人的奖励值。</li> <li>让机器人学会逃出迷宫：逃出迷宫前的每一步都提供 -1 作为「奖励值」。</li> </ul> <p>不难理解，如果提供正值作为奖励，则会不断「鼓励」机器人多走路，反之，则会「促使」机器人找到一条最短路径逃出迷宫。</p> <p>前面说到，agent 的目标很清晰，就是最大化累积奖励值，而他的行动也很大程度受到我们给定的 rewards 的影响，不恰当地设定 rewards 必然会影响到 agent 的学习效果。所以，需要强调一点，我们要提供的 rewards ，是要用于<strong>促使</strong> agent 来<strong>达成目标</strong>，而不是根据人的先验知识来<strong>告诉他该怎样去做</strong>。说直白点，就是不要去人为「干预」他怎样学，只管给他设定目标和一些基本规则，让他多去自由探索。</p> <p>举个例子，在下棋时，应该在真正赢下比赛时才给予适当奖励，而不是在吃掉对手某个棋子，亦或占据某个区域时就立即给他奖励。虽然按人的理解，吃掉棋子之类的行动一般都看似是个不错的选择，但在一些关键的步骤，说不定以退为进才是上策，盲目吃子反而陷入陷阱。所以如果按后者那样的策略去学习，agent 最终总会倾向于不顾输赢地去拿下这些眼前利益，而不去长远考虑，失去「大局观」。</p> <p>之前强调过，rewards 的计算要放在 agent 外界，现在不难明白，原因就在于 agent 的终极目标是要去<strong>掌握之前并不能完美掌握的事物</strong>，而这正是我们设计强化学习来解决难题的本质。</p> <h2 id=33-returns>3.3 Returns<a class=headerlink href=#33-returns title="Permanent link">&para;</a></h2> <p>目前已经明确了强化学习的目标——最大化累积奖励，那么如何用数学语言来表述呢？这里我们引入一个返回值（Return）的概念，定义为<strong>奖励值序列的特定函数</strong>。定义 <span class=arithmatex>\(G_t\)</span> 来表示时间点 t 之后的期望返回值，一种最简单的期望返回值为对未来的奖励值序列进行求和：</p> <div class=arithmatex>\[G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_T\]</div> <h3 id=episodic-tasks>Episodic Tasks<a class=headerlink href=#episodic-tasks title="Permanent link">&para;</a></h3> <p>上述的返回值表示形式只在有限时间点条件（或者说有终止时间点）下适用，我们将这类任务定义为<strong>片段式任务</strong>：</p> <ul> <li>episodes: 决策体与环境的交互过程的子片段（任意重复性的交互过程），称为 episodes</li> <li>terminal state: 每个子片段都有一个特殊状态，其后续时间点被重置进入新片段，这样的特殊片段，称为 terminal state</li> </ul> <p>在片段式任务中时常需要区分终止态，所以符号上也要有所区分：</p> <ul> <li><span class=arithmatex>\(\mathcal{S}\)</span>: 非终止态的全体集合</li> <li><span class=arithmatex>\(\mathcal{S}^+\)</span>: <span class=arithmatex>\(\mathcal{S} \bigcup\)</span> {终止态全体}</li> </ul> <h3 id=continuing-tasks>Continuing Tasks<a class=headerlink href=#continuing-tasks title="Permanent link">&para;</a></h3> <p>在很多时候，agent 与 environment 的交互过程并不能很自然地被分解为「片段」，而是无止限地持续下去，称之为连续式任务。此时上面的 <span class=arithmatex>\(G_t\)</span> 会因 <span class=arithmatex>\(T=\infty\)</span> 而必然趋于无穷，导致 agent 无法根据返回值来进行比较学习，此时需要加入「削减系数」<span class=arithmatex>\(\gamma\)</span> 。</p> <p><strong>削减返回值</strong>:</p> <div class=arithmatex>\[G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\]</div> <p>其中 <span class=arithmatex>\(\gamma(0\leq \gamma \leq 1)\)</span> 作为削减率，决定了未来奖励值在当前的表现：</p> <ul> <li>若 <span class=arithmatex>\(\gamma \rightarrow 0\)</span>，agent 更着重于最大化即时奖励值，也就是说，此时他的目标为学习如何选择 <span class=arithmatex>\(A_t\)</span> 来最大化 <span class=arithmatex>\(R_{t+1}\)</span>.</li> <li>若 <span class=arithmatex>\(\gamma \rightarrow 1\)</span>，返回值会更加强烈地把未来的奖励情况考虑进来，使得 agent 变得更有「远见」。</li> </ul> <h2 id=34-unified-notation-for-episodic-and-continuing-tasks>3.4 Unified Notation for Episodic and Continuing Tasks<a class=headerlink href=#34-unified-notation-for-episodic-and-continuing-tasks title="Permanent link">&para;</a></h2> <p>在后面的讨论中，我们会对各种任务来统一地来讨论分析，所以需要统一符号。</p> <p>在片段式任务中，我们应该用 <span class=arithmatex>\(S_{t,i}\)</span> 来表示第 i 段的第 t 步（ <span class=arithmatex>\(A_{t,i}, R_{t,i}, \pi_{t,i}, T_i\)</span> 等同理），但是每一段其实本质上意义相近，对于其他段我们可以统一分析，故简写 <span class=arithmatex>\(S_t\)</span> 来统一表示各个 <span class=arithmatex>\(S_{t,i}\)</span>，其他几个符号也同理。</p> <p>此外，我们还需要统一片段式任务和连续式任务的形式，他们分布有着有限项的返回值公式和无限项的返回值公式，为了统一公式，我们针对片段式任务加入一种特殊的「吸收态」，其特点是状态的交互和转移过程都只在自身进行，且奖励值为 0 ，如下图所示：</p> <p><img alt="Absorbing State" src=../imgs/RLAI_3/unified.png></p> <p>这样显然可见，片段式任务也能表示为无穷项了，只不过原本的终止态之后 reward 为 0，不对公式造成影响。于是我们可以统一地定义：</p> <div class=arithmatex>\[G_t\doteq \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k\]</div> <h2 id=35-the-markov-property>3.5 The Markov Property<a class=headerlink href=#35-the-markov-property title="Permanent link">&para;</a></h2> <p>回想我们进行强化学习的主要目标在于，学习出一个能由任意状态信号决定行动的策略函数，而状态是以<strong>即时感知以及历史状态和信息</strong>为基础，逐渐<strong>构筑、维护</strong>下来的，他包含一定的历史信息，但并不意味着能从中得知环境中的一切信息。比如棋盘上的棋子，棋子在不同的位置即在不同的状态，从每个状态，我们能知道走下这步棋各种可能的组合，但并不能准确得知他是如何一步一步走到这一步的。</p> <p>正如上面的棋子的例子一样，我们称，如果一个状态继承并保留了所有相关信息，则具有<strong>马尔可夫性质</strong>。下面在用数学语言描述一下这一性质。</p> <h3 id=markov-property>Markov Property<a class=headerlink href=#markov-property title="Permanent link">&para;</a></h3> <p>首先定义基于全部历史信息的完全联合分布：</p> <div class=arithmatex>\[\mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_{t-1},A_{t-1},R_t,S_t,A_t\}\]</div> <p>环境的动态分布定义为：</p> <div class=arithmatex>\[p(s',r|,s,a)\doteq \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a\}\]</div> <p>当且仅当上面两式相等，即</p> <div class=arithmatex>\[p(s',r|s,a) = \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_t,A_t\}\]</div> <p>我们称这样的环境和任务具有<strong>马尔可夫性质</strong>。</p> <p>马尔科夫性质在强化学习中的重要之处体现在，如果问题具备这一条件，那么各种决策值将只与当前状态有关，这能极大程度地方便我们分析各种理论和模型。即使是非严格遵守马尔可夫性质的问题，也能有所应用。</p> <p><em>这本书之后的所有理论都将基于马尔可夫性质来讨论。</em></p> <h2 id=36-markov-decision-processes>3.6 Markov Decision Processes<a class=headerlink href=#36-markov-decision-processes title="Permanent link">&para;</a></h2> <p>如果一个强化学习任务满足马尔可夫性质，我们称之为<strong>马尔可夫决策过程（MDP）</strong>。如果状态空间和行动空间都是有限的，则称为<strong>有限马尔可夫过程（finite MDP）</strong>。</p> <p>在有限马尔可夫过程中，状态集、行动集、奖励集（<span class=arithmatex>\(\mathcal{S}, \mathcal{A}, \mathcal{R}\)</span>）内的元素均有上界，所以能定义</p> <div class=arithmatex>\[p(s',r|s,a)\doteq\mathrm{Pr}\left\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\right\}\]</div> <p>其中</p> <div class=arithmatex>\[\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1\]</div> <p>有了 <span class=arithmatex>\(p(s',r|s,a)\)</span> ，我们可以进一步计算得到：</p> <ul> <li><strong>状态转移概率</strong>:</li> </ul> <div class=arithmatex>\[p(s'|s,a)\doteq\mathrm{Pr}\left\{S_t=s'|S_{t-1}=s,A_{t-1}=a\right\}=\sum_{r\in\mathcal{R}}p(s',r|s,a)\]</div> <ul> <li><strong>给定状态、行动的期望奖励</strong>:</li> </ul> <div class=arithmatex>\[r(s,a)\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)\]</div> <ul> <li><strong>给定状态、行动、后继状态的期望奖励</strong>:</li> </ul> <div class=arithmatex>\[r(s,a,s')\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s'\right]=\sum_{r\in\mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)}\]</div> <p>得到这些动态性质后，有助于我们后面的具体分析。</p> <h3 id=example-recycling-robot-mdp>Example: Recycling Robot MDP<a class=headerlink href=#example-recycling-robot-mdp title="Permanent link">&para;</a></h3> <p>前面提到的清洁机器人，便能按照有限马尔科夫决策过程来分析。下面的图表详细地描述了怎样用 MDP 来分析问题。比如第三、四行，表示机器人在低电量下仍去搜索垃圾，有 <span class=arithmatex>\(\beta\)</span> 的概率仍然是低电量，收获 <span class=arithmatex>\(r_{search}\)</span> 的奖励值，有 <span class=arithmatex>\(\beta​\)</span> 的概率耗尽电量未能及时自行返回充电，被人工带回充电后进入高电量状态，同时给予 -3 的惩罚值。其他的过程也可类似分析，可以看出在分析接下来情景时，我们无需考虑机器人过去所有做过的事情，只需要从当前状态出发，分析进入下一个可能状态的过程即可。</p> <p><img alt=robot_tb src=../imgs/RLAI_3/robot_tb.png></p> <p><img alt=robot_diag src=../imgs/RLAI_3/robot_diag.png></p> <h2 id=37-value-functions>3.7 Value Functions<a class=headerlink href=#37-value-functions title="Permanent link">&para;</a></h2> <p>强化学习算法总是涉及到估计 value function，用于量化评估不同条件下行动的好坏程度。这里我们可以对返回值求期望来作为 value function 来进行评估，而显然 agent 的策略决定了如何来计算这一期望。</p> <p>回想我们反复说到的策略这一概念，简单来说，策略是由状态空间 <span class=arithmatex>\(\mathcal{S}\)</span>、行动空间 <span class=arithmatex>\(\mathcal{A}\)</span> 到概率空间的一个映射：<span class=arithmatex>\(\pi:\mathcal{S}\times\mathcal{A}\to \mathcal{P}\)</span> 。</p> <ul> <li><span class=arithmatex>\(\pi(a|s)\)</span>: 状态为 s 时选择行动 a 的概率</li> <li><span class=arithmatex>\(v_{\pi}(s)\)</span>: <strong>策略 <span class=arithmatex>\(\pi\)</span> 下的状态值函数</strong>。表示状态 s 下，遵守策略 <span class=arithmatex>\(\pi\)</span> 的期望返回值。对于 MDP，我们可以定义</li> </ul> <div class=arithmatex>\[v_{\pi}(s)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s\right], \forall s \in \mathcal{S}\]</div> <ul> <li><span class=arithmatex>\(q_{\pi}(s,a)\)</span>: <strong>策略 <span class=arithmatex>\(\pi\)</span> 下的行动值函数</strong>。表示状态 s 下，选择行动 a ，遵守策略 <span class=arithmatex>\(\pi\)</span> 的期望返回值。类似地，可以定义</li> </ul> <div class=arithmatex>\[q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s,A_t=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s,A_t=a\right]\]</div> <h3 id=monte-carlo-methods><strong>Monte Carlo Methods</strong><a class=headerlink href=#monte-carlo-methods title="Permanent link">&para;</a></h3> <p>上面的函数可以通过模拟方法来进行经验估计，称为蒙特卡洛方法。</p> <ul> <li>在实验中，若 agent 在遵守策略 <span class=arithmatex>\(\pi\)</span> 的条件下，为每个出发状态 s 都保留了后面的真实返回值的均值，显然这个均值会随实验次数增加而最终收敛到 <span class=arithmatex>\(v_{\pi}(s)\)</span>.</li> <li>类似地，如果还细分到为每个出发状态 s、每个行动选择 a 都保留了真实返回值的均值，则便能最终收敛到 <span class=arithmatex>\(q_{\pi}(s,a)\)</span>.</li> </ul> <p>如果状态空间很大，显然可知，进行上述的大量样本实验来模拟估计很不现实。此时可以考虑将 <span class=arithmatex>\(v_{\pi}, q_{\pi}\)</span> 看作参数化函数，通过调参来逼近真实值，一样能够较为精确地进行估计（第五章讲）。</p> <h3 id=bellman-equation>Bellman Equation<a class=headerlink href=#bellman-equation title="Permanent link">&para;</a></h3> <p>Value function 被用在强化学习和动态规划的一个基本性质在于其满足一种特殊递归关系：</p> <div class=arithmatex>\[\begin{aligned} v_\pi(s) &amp;\doteq \mathbb{E}_\pi[G_t\,|\,S_t=s] \\ &amp;=\mathbb{E}\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\,|\,S_t=s \right] \\ &amp;= \mathbb{E}_\pi \left [R_{t+1}+\gamma\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_t=s \right] \\ &amp;=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\left[r+\gamma \mathbb{E}_\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_{t+1}=s' \right] \right] \\ &amp;= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right], \qquad \forall s \in \mathcal S \end{aligned}\]</div> <ul> <li><span class=arithmatex>\(a\)</span>: 一个行动，取自行动集 <span class=arithmatex>\(\mathcal{A}(s)\)</span></li> <li><span class=arithmatex>\(s'\)</span>: 后继状态，取自状态集 <span class=arithmatex>\(\mathcal{S}\)</span> （对于片段式任务，取自 <span class=arithmatex>\(\mathcal{S}^+\)</span> ）</li> <li><span class=arithmatex>\(r\)</span>: 奖励值，取自奖励集 <span class=arithmatex>\(\mathcal{R}\)</span></li> </ul> <p>对于任意策略 <span class=arithmatex>\(\pi\)</span> 和状态 s ，当前 value 和未来可能的 value 满足上面的递推关系，称为<strong>贝尔曼方程（Bellman Equation）</strong>。</p> <p><strong><span class=arithmatex>\(v_\pi\)</span> 的贝尔曼方程：</strong></p> <blockquote> <div class=arithmatex>\[v_{\pi}(s) = \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right]\]</div> </blockquote> <p>贝尔曼方程描述了<strong>状态值与所有后继状态值的关系</strong>。</p> <p><img alt=back-diag-v src=../imgs/RLAI_3/back-diag-v.png></p> <p>如上面这样的示意图，我们称为「<strong>backup diagrams</strong>」，它能描绘出问题的更新反馈机制，这也正是强化学习算法的一个核心之处。</p> <h3 id=example-gridworld>Example: Gridworld<a class=headerlink href=#example-gridworld title="Permanent link">&para;</a></h3> <p><img alt=grid-world src=../imgs/RLAI_3/grid-world.png></p> <p>在这个例子中，每一格上有四个等概率行动（往四个方向移动）。若从靠边的格子往界外移动，则会给予 -1 的惩罚，但位置保持不变，若移入特殊点 <span class=arithmatex>\(A,B\)</span> ，则分别被移入 <span class=arithmatex>\(A',B'\)</span> 并给予 +10、+5 的特殊奖励，其余情况的奖励为 0 。</p> <p>右图则是通过解贝尔曼线性方程组求出了 <span class=arithmatex>\(v_{\pi}\)</span> 的解并填写在格子上（每个格子分别对应一个状态 <span class=arithmatex>\(s_i\)</span>，填写的值则为 <span class=arithmatex>\(v_{\pi}(s_i)\)</span> ）。本例中，<span class=arithmatex>\(\gamma = 0.9\)</span>。</p> <h2 id=38-optimal-value-functions>3.8 Optimal Value Functions<a class=headerlink href=#38-optimal-value-functions title="Permanent link">&para;</a></h2> <p>解决强化学习问题就是在于找到一个最优的测量，使 agent 能够按照该策略行动并得到最好的累积奖励值，在 MDP 中，当且仅当一个策略 <span class=arithmatex>\(\pi​\)</span> 在任意状态下的期望返回值都大于等于策略 <span class=arithmatex>\(\pi'​\)</span> 的期望返回值，称策略 <span class=arithmatex>\(\pi​\)</span> 优于策略 <span class=arithmatex>\(\pi'​\)</span> ，即： <span class=arithmatex>\(\pi \geq \pi'​\)</span> 当且仅当 <span class=arithmatex>\(v_{\pi}(s)\geq v_{\pi'}(s), \forall s \in \mathcal{S}​\)</span>.</p> <ul> <li><strong>最优策略</strong>：至少存在一个策略，优于其他所有策略，称所有这样的策略为最优策略</li> <li><strong>最优状态值函数</strong>：<span class=arithmatex>\(v_*(s)\doteq \max\limits_{\pi}v_{\pi}(s),\forall s \in \mathcal{S}\)</span></li> <li><strong>最优行动值函数</strong>：<span class=arithmatex>\(q_*(s,a)=\max\limits_{\pi}q_{\pi}(s,a),\forall s \in \mathcal{S}\)</span></li> </ul> <p>其中易知，<span class=arithmatex>\(q_*(s,a)=\mathbb{E}\left[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a\right]\)</span></p> <h3 id=bellman-optimality-equation>Bellman Optimality Equation<a class=headerlink href=#bellman-optimality-equation title="Permanent link">&para;</a></h3> <p><span class=arithmatex>\(v_*\)</span> 作为 value function，也有贝尔曼方程，此时称该方程为贝尔曼最优方程。</p> <p>贝尔曼最优方程表述了<strong>最优策略下一个状态的状态值，必然等于该状态下最优行动的期望返回值</strong>：</p> <div class=arithmatex>\[\begin{aligned}v_*(s) &amp;= \max_{a\in \mathcal A(s)}q_{\pi_*}(s,a) \\ &amp;=\max_a \mathbb{E}_{\pi_*} \left[G_t \mid S_t=s, A_t=a \right] \\ &amp;= \max_a \mathbb{E}_{\pi_*} \left[\sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t=s,A_t=a \right] \\ &amp;= \max_a \mathbb{E}_{\pi_*} \left[R_{t+1}+\gamma \sum_{k=0}^\infty\gamma^kR_{t+k+2}\mid S_t=s,A_t=a \right] \\ &amp;= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a] \\ &amp;=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}\]</div> <p><strong>贝尔曼最优方程</strong>:</p> <ul> <li><span class=arithmatex>\(v_*\)</span>:</li> </ul> <blockquote> <div class=arithmatex>\[\begin{aligned} v_*(s)&amp;= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a]\\&amp;=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}\]</div> </blockquote> <ul> <li><span class=arithmatex>\(q_*\)</span>:</li> </ul> <blockquote> <div class=arithmatex>\[\begin{aligned}q_*(s,a) &amp;= \mathbb{E} \left[R_{t+1}+\gamma \max_{a'}q_*(S_{t+1},a') \mid S_t=s,A_t=a \right] \\ &amp;= \sum_{s',r}p(s',r \mid s,a)\left[r+\gamma \max_{a'}q_*(s',a') \right]\end{aligned}\]</div> </blockquote> <p><img alt=opt-diag src=../imgs/RLAI_3/opt-diag.png></p> <p>在上面的 backup diagrams 中，我们用一个圆弧来表示在不同的选项中去最大值。</p> <p>对于有限 MDP ，贝尔曼最优方程一定有独立于策略的唯一解。事实上，设有 n 个状态，每个状态对应一个方程，这样总共由 n 个不同的方程形成 n 元方程组，如果环境的动态 <span class=arithmatex>\(p(s',r|s,a)\)</span> 给定，由数学知识可知，该方程组必然能得到唯一解 <span class=arithmatex>\(v_*\)</span> ，进而又可利用其计算得到 <span class=arithmatex>\(q_*\)</span> 。</p> <p>一旦求得这一解，便能确定最优策略：</p> <blockquote> <p><strong>最优策略</strong>：显然至少存在一个行动能使行动值取到 <span class=arithmatex>\(v_*\)</span> ，如果一个策略只将非 0 概率分配给这样的行动，称这个策略是最优策略。</p> </blockquote> <p>如果每一步都采取<strong>贪心策略</strong>，即只根据 <span class=arithmatex>\(v_*\)</span> 来确定下一步的行动，这样的行动却恰好是最优行动，<span class=arithmatex>\(v_*\)</span> 的优美之处便体现于此。之所以能达到这一效果，是因为 <span class=arithmatex>\(v_*\)</span> 已经考虑到了未来所有可能性，于是，看似贪心的「一步搜索」却能生成出全局最优行动。</p> <p>如果我们进一步解得了 <span class=arithmatex>\(q_*\)</span> ，agent 甚至都无需来做「一步搜索」：对于任意状态 s ，只需找到 <span class=arithmatex>\(a_0\)</span> 使得 <span class=arithmatex>\(q_*(s,a_0)=\max\limits_{a}q_*(s,a)\)</span> 即可。这是因为我们已经在之前的工作中多做了一些准备，将进一步的搜索信息缓存在了各个 <span class=arithmatex>\(q_*\)</span> 中，使得它的信息量比 <span class=arithmatex>\(v_*\)</span> 更大。</p> <h3 id=example-solving-the-gridworld>Example: Solving the Gridworld<a class=headerlink href=#example-solving-the-gridworld title="Permanent link">&para;</a></h3> <p><img alt=grid-world src=../imgs/RLAI_3/grid-world_solve.png></p> <ul> <li>中图：最优状态值函数</li> <li>右图：最优策略</li> </ul> <p>显式求解贝尔曼最优方程虽然能直接确定最优策略，但是并不太实用，因为这一方法涉及到了「穷举」，依赖严苛的计算资源和内存条件。此外，这个方法还依赖于三个假设，使得问题本身就很难利用上这一方法：</p> <ol> <li>精确地知道环境的动态性质 <span class=arithmatex>\(p\)</span></li> <li>有足够的计算资源</li> <li>满足马尔可夫性质</li> </ol> <p>在后面的章节中会介绍一些算法，如「启发式搜索」、「动态规划」等，可以看作是对「解贝尔曼方程」的近似求解，能够弥补上述的一些不足之处。</p> <h2 id=39-optimality-and-approximation>3.9 Optimality and Approximation<a class=headerlink href=#39-optimality-and-approximation title="Permanent link">&para;</a></h2> <p>前面已经讲过，最优策略是通过耗费极端的计算资源求得的，这使得我们不得不考虑一些方法来近似估计前面的一些函数。</p> <p>在近似求解最优行为时，我们不难想象，会有很多状态其实只会以极低概率出现，我们若仍对其求解最优行动则意义不大，这时候如果选取一个局部最优行动来代替最优行动，显然，从整体期望意义来看，对总体奖励值造成的影响其实并不大，但却能节省很多的计算资源。与之对应的，当遇到那些经常出现的状态，我们则务必求出最优解。这是在解决 MDP 时强化学习方法区别于其他近似方法的一个重要性质。</p> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">July 26, 2020</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../RLAI_2/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Chapter 2" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> Chapter 2 </div> </div> </a> <a href=../RLAI_4/ class="md-footer__link md-footer__link--next" aria-label="Next: Chapter 4" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> Chapter 4 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016-2023 ZHANGWP </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/zawnpn target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/zawnpn target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://www.linkedin.com/in/zawnpn/ target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href=https://www.zhihu.com/people/zhangwanpeng target=_blank rel=noopener title=www.zhihu.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13H170.54zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82v170.31zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62v.01zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2l19.23 14.43zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78l.03-.01z"/></svg> </a> <a href=https://psnprofiles.com/zawnpn target=_blank rel=noopener title=psnprofiles.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M570.9 372.3c-11.3 14.2-38.8 24.3-38.8 24.3L327 470.2v-54.3l150.9-53.8c17.1-6.1 19.8-14.8 5.8-19.4-13.9-4.6-39.1-3.3-56.2 2.9L327 381.1v-56.4c23.2-7.8 47.1-13.6 75.7-16.8 40.9-4.5 90.9.6 130.2 15.5 44.2 14 49.2 34.7 38 48.9zm-224.4-92.5v-139c0-16.3-3-31.3-18.3-35.6-11.7-3.8-19 7.1-19 23.4v347.9l-93.8-29.8V32c39.9 7.4 98 24.9 129.2 35.4C424.1 94.7 451 128.7 451 205.2c0 74.5-46 102.8-104.5 74.6zM43.2 410.2c-45.4-12.8-53-39.5-32.3-54.8 19.1-14.2 51.7-24.9 51.7-24.9l134.5-47.8v54.5l-96.8 34.6c-17.1 6.1-19.7 14.8-5.8 19.4 13.9 4.6 39.1 3.3 56.2-2.9l46.4-16.9v48.8c-51.6 9.3-101.4 7.3-153.9-10z"/></svg> </a> <a href=https://steamcommunity.com/id/zawnpn/ target=_blank rel=noopener title=steamcommunity.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M496 256c0 137-111.2 248-248.4 248-113.8 0-209.6-76.3-239-180.4l95.2 39.3c6.4 32.1 34.9 56.4 68.9 56.4 39.2 0 71.9-32.4 70.2-73.5l84.5-60.2c52.1 1.3 95.8-40.9 95.8-93.5 0-51.6-42-93.5-93.7-93.5s-93.7 42-93.7 93.5v1.2L176.6 279c-15.5-.9-30.7 3.4-43.5 12.1L0 236.1C10.2 108.4 117.1 8 247.6 8 384.8 8 496 119 496 256zM155.7 384.3l-30.5-12.6a52.79 52.79 0 0 0 27.2 25.8c26.9 11.2 57.8-1.6 69-28.4 5.4-13 5.5-27.3.1-40.3-5.4-13-15.5-23.2-28.5-28.6-12.9-5.4-26.7-5.2-38.9-.6l31.5 13c19.8 8.2 29.2 30.9 20.9 50.7-8.3 19.9-31 29.2-50.8 21zm173.8-129.9c-34.4 0-62.4-28-62.4-62.3s28-62.3 62.4-62.3 62.4 28 62.4 62.3-27.9 62.3-62.4 62.3zm.1-15.6c25.9 0 46.9-21 46.9-46.8 0-25.9-21-46.8-46.9-46.8s-46.9 21-46.9 46.8c.1 25.8 21.1 46.8 46.9 46.8z"/></svg> </a> <a href=https://space.bilibili.com/38332230 target=_blank rel=noopener title=space.bilibili.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M512 256c0 141.4-114.6 256-256 256S0 397.4 0 256 114.6 0 256 0s256 114.6 256 256zm-336-88v176c0 8.7 4.7 16.7 12.3 20.9 7.5 4.3 16.8 4.1 24.2-.4l144-88c7.1-4.4 11.5-12.1 11.5-20.5s-4.4-16.1-11.5-20.5l-144-88c-7.4-5.4-16.7-4.7-24.2-.4-7.6 4.2-12.3 12.2-12.3 20.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../..", "features": ["navigation.tabs", "navigation.prune"], "search": "../../../../assets/javascripts/workers/search.b97dbffb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script> <script src=../../../../assets/javascripts/bundle.6c7ad80a.min.js></script> <script src=../../../../_static/extra.js></script> <script src=//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js></script> <script src=//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script> </body> </html>